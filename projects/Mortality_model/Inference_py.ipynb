{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vchaudha\\AppData\\Local\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#!{sys.executable} -m pip install pandas numpy scikit-learn lightgbm matplotlib duckdb \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import warnings\n",
    "import duckdb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (confusion_matrix, auc, roc_curve, accuracy_score, \n",
    "                             precision_score, recall_score, f1_score, \n",
    "                             precision_recall_curve, roc_auc_score, brier_score_loss)\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "#pd.set_option('display.max_rows', None)\n",
    "random.seed(37)\n",
    "np.random.seed(37)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "con = duckdb.connect(database=\":memory:\")\n",
    "\n",
    "print(\"Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### control panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_location=\"C:/Users/vchaudha/OneDrive - rush.edu/CLIF-1.0-main\"\n",
    "\n",
    "race_map = {\n",
    "    'White': 'White',\n",
    "    'Black or African American': 'Black',\n",
    "    'Asian': 'Asian',\n",
    "    'Other': 'Others',\n",
    "    'Unknown': 'Others',\n",
    "    'Did Not Encounter': 'Others',\n",
    "    'Refusal': 'Others',\n",
    "    'American Indian or Alaska Native': 'Others',\n",
    "    'Native Hawaiian or Other Pacific Islander': 'Others',\n",
    "    np.nan: 'Others'\n",
    "}\n",
    "\n",
    "ethnicity_map = {\n",
    "    'Not Hispanic or Latino': 'Not Hispanic or Latino',\n",
    "    'Hispanic or Latino': 'Hispanic or Latino',\n",
    "    'Did Not Encounter': 'Others',\n",
    "    'Refusal': 'Others',\n",
    "    '*Unspecified': 'Others',\n",
    "    np.nan: 'Others'\n",
    "}\n",
    "\n",
    "site_name='RUSH'\n",
    "\n",
    "finetune=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICU close to Admission\n",
    "\n",
    "1. Check ICU location_category between admission_dttmtime and 48hr stop from admission\n",
    "2. Check ICU stay at least 24 hr (for ICU - OR - ICU including OR in ICU stay 24hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = pd.read_csv(f\"{tables_location}/rclif/clif_adt.csv\")\n",
    "encounter = pd.read_csv(f\"{tables_location}/rclif/clif_encounter_demographics_dispo.csv\")\n",
    "limited=pd.read_csv(f\"{tables_location}/rclif/clif_limited_identifiers.csv\")\n",
    "demog=pd.read_csv(f\"{tables_location}/rclif/clif_patient_demographics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "join=pd.merge(location[['encounter_id','location_category','in_dttm','out_dttm']],\\\n",
    "              limited[['encounter_id','admission_dttm']], on=['encounter_id'], how='left')\n",
    "\n",
    "icu_data=pd.merge(join,\\\n",
    "                  encounter[['encounter_id','age_at_admission','disposition']], on=['encounter_id'], how='left')\n",
    "\n",
    "\n",
    "icu_data['in_dttm'] = pd.to_datetime(icu_data['in_dttm'])\n",
    "icu_data['admission_dttm'] = pd.to_datetime(icu_data['admission_dttm'])\n",
    "icu_data['out_dttm'] = pd.to_datetime(icu_data['out_dttm'])\n",
    "\n",
    "icu_48hr_check = icu_data[\n",
    "    (icu_data['location_category'] == 'ICU') &\n",
    "    (icu_data['in_dttm'] >= icu_data['admission_dttm']) &\n",
    "    (icu_data['in_dttm'] <= icu_data['admission_dttm'] + pd.Timedelta(hours=48)) &\n",
    "    (icu_data['in_dttm'].dt.year >= 2020) & (icu_data['in_dttm'].dt.year <= 2022)\n",
    "]['encounter_id'].unique()\n",
    "\n",
    "icu_data=icu_data[icu_data['encounter_id'].isin(icu_48hr_check) & (icu_data['in_dttm'] <= icu_data['admission_dttm'] + pd.Timedelta(hours=72))].reset_index(drop=True)\n",
    "\n",
    "icu_data = icu_data.sort_values(by=['in_dttm']).reset_index(drop=True)\n",
    "\n",
    "icu_data[\"RANK\"]=icu_data.sort_values(by=['in_dttm'], ascending=True).groupby(\"encounter_id\")[\"in_dttm\"].rank(method=\"first\", ascending=True).astype(int)\n",
    "\n",
    "\n",
    "min_icu=icu_data[icu_data['location_category'] == 'ICU'].groupby('encounter_id')['RANK'].min()\n",
    "icu_data=pd.merge(icu_data, pd.DataFrame(zip(min_icu.index, min_icu.values), columns=['encounter_id', 'min_icu']), on='encounter_id', how='left')\n",
    "icu_data=icu_data[icu_data['RANK']>=icu_data['min_icu']].reset_index(drop=True)\n",
    "\n",
    "icu_data.loc[icu_data['location_category'] == 'OR', 'location_category'] = 'ICU'\n",
    "\n",
    "icu_data['group_id'] = (icu_data.groupby('encounter_id')['location_category'].shift() != icu_data['location_category']).astype(int)\n",
    "icu_data['group_id'] = icu_data.sort_values(by=['in_dttm'], ascending=True).groupby('encounter_id')['group_id'].cumsum()\n",
    "\n",
    "\n",
    "icu_data = icu_data.sort_values(by=['in_dttm'], ascending=True).groupby(['encounter_id', 'location_category', 'group_id']).agg(\n",
    "    min_in_dttm=('in_dttm', 'min'),\n",
    "    max_out_dttm=('out_dttm', 'max'),\n",
    "    admission_dttm=('admission_dttm', 'first'),\n",
    "    age=('age_at_admission', 'first'),\n",
    "    dispo=('disposition', 'first')\n",
    ").reset_index()\n",
    "\n",
    "min_icu=icu_data[icu_data['location_category'] == 'ICU'].groupby('encounter_id')['group_id'].min()\n",
    "icu_data=pd.merge(icu_data, pd.DataFrame(zip(min_icu.index, min_icu.values), columns=['encounter_id', 'min_icu']), on='encounter_id', how='left')\n",
    "\n",
    "icu_data=icu_data[(icu_data['min_icu']==icu_data['group_id']) &\n",
    "         (icu_data['max_out_dttm']-icu_data['min_in_dttm'] >= pd.Timedelta(hours=24))\n",
    "         ].reset_index(drop=True)\n",
    "\n",
    "\n",
    "icu_data['after_24hr']=icu_data['min_in_dttm'] + pd.Timedelta(hours=24)\n",
    "\n",
    "icu_data=icu_data[['encounter_id','min_in_dttm','after_24hr','age','dispo']]\n",
    "\n",
    "icu_data=pd.merge(icu_data,\\\n",
    "                  demog, on=['encounter_id'], how='left')[['encounter_id','min_in_dttm','after_24hr','age','dispo','sex','ethnicity','race']]\n",
    "icu_data=icu_data[~icu_data['sex'].isna()].reset_index(drop=True)\n",
    "icu_data['isfemale']=(icu_data['sex'].str.lower() == 'female').astype(int)\n",
    "icu_data['isdeathdispo'] = (icu_data['dispo'].str.contains('dead|expired', case=False, regex=True)).astype(int)\n",
    "\n",
    "icu_data['ethnicity'] = icu_data['ethnicity'].map(ethnicity_map)\n",
    "icu_data['race'] = icu_data['race'].map(race_map)\n",
    "\n",
    "\n",
    "del location,encounter,limited,demog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals = con.execute(f'''\n",
    "    SELECT \n",
    "        encounter_id,\n",
    "        CAST(recorded_dttm AS datetime) AS recorded_time,\n",
    "        CAST(vital_value AS float) AS vital_value,\n",
    "        vital_name \n",
    "    FROM \n",
    "        read_csv_auto('{tables_location}/rclif/clif_vitals.csv')\n",
    "    WHERE \n",
    "        vital_name IN ('weight_kg', 'pulse', 'sbp', 'dbp', 'temp_c','height_inches') \n",
    "        AND encounter_id IN (SELECT DISTINCT encounter_id FROM icu_data);\n",
    "''').df()\n",
    "\n",
    "vitals=con.execute('''\n",
    "PIVOT vitals\n",
    "ON vital_name\n",
    "USING first(vital_value)\n",
    "GROUP BY encounter_id,recorded_time;\n",
    "''').df()\n",
    "\n",
    "vitals['height_meters'] = vitals['height_inches'] * 0.0254\n",
    "\n",
    "# Calculate BMI\n",
    "vitals['bmi'] = vitals['weight_kg'] / (vitals['height_meters'] ** 2)\n",
    "\n",
    "\n",
    "icu_data_agg=pd.merge(icu_data,vitals, on=['encounter_id'], how='left')\n",
    "icu_data_agg=icu_data_agg[(icu_data_agg['recorded_time'] >= icu_data_agg['min_in_dttm']) & (icu_data_agg['recorded_time'] <= icu_data_agg['after_24hr'])].reset_index(drop=True)\n",
    "\n",
    "icu_data_agg = icu_data_agg.groupby(['encounter_id']).agg(\n",
    "    min_bmi=('bmi', 'min'),\n",
    "    max_bmi=('bmi', 'max'),\n",
    "    avg_bmi=('bmi', 'mean'),\n",
    "    min_weight_kg=('weight_kg', 'min'),\n",
    "    max_weight_kg=('weight_kg', 'max'),\n",
    "    avg_weight_kg=('weight_kg', 'mean'),\n",
    "    min_pulse=('pulse', 'min'),\n",
    "    max_pulse=('pulse', 'max'),\n",
    "    avg_pulse=('pulse', 'mean'),\n",
    "    min_sbp=('sbp', 'min'),\n",
    "    max_sbp=('sbp', 'max'),\n",
    "    avg_sbp=('sbp', 'mean'),\n",
    "    min_dbp=('dbp', 'min'),\n",
    "    max_dbp=('dbp', 'max'),\n",
    "    avg_dbp=('dbp', 'mean'),\n",
    "    min_temp_c=('temp_c', 'min'),\n",
    "    max_temp_c=('temp_c', 'max'),\n",
    "    avg_temp_c=('temp_c', 'mean'),\n",
    ").reset_index()\n",
    "\n",
    "icu_data=pd.merge(icu_data,icu_data_agg, on=['encounter_id'], how='left')\n",
    "\n",
    "del vitals,icu_data_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = con.execute(f'''\n",
    "    SELECT \n",
    "        encounter_id,\n",
    "        CAST(lab_order_dttm AS datetime) AS lab_order_dttm,\n",
    "        TRY_CAST(lab_value AS float) AS lab_value,\n",
    "        lab_name \n",
    "    FROM \n",
    "        read_csv_auto('{tables_location}/rclif/clif_labs.csv')\n",
    "    WHERE \n",
    "        ((lab_name='monocyte' and reference_unit = '%' and lab_type_name='standard') OR\n",
    "        (lab_name='lymphocyte' and reference_unit = '%' and lab_type_name='standard') OR\n",
    "        (lab_name='basophil' and reference_unit = '%' and lab_type_name='standard') OR\n",
    "        (lab_name='neutrophil' and reference_unit = '%' and lab_type_name='standard') OR\n",
    "        (lab_name='albumin' and reference_unit = 'g/dL' and lab_type_name='standard') OR\n",
    "        (lab_name='ast' and reference_unit = 'U/L' and lab_type_name='standard') OR\n",
    "        (lab_name='total_protein' and reference_unit = 'g/dL' and lab_type_name='standard') OR\n",
    "        (lab_name='alkaline_phosphatase' and reference_unit = 'U/L' and lab_type_name='standard') OR\n",
    "        (lab_name='bilirubin_total' and reference_unit = 'mg/dL' and lab_type_name='standard') OR\n",
    "        (lab_name='bilirubin_conjugated' and reference_unit = 'mg/dL' and lab_type_name='standard') OR\n",
    "        (lab_name='calcium' and reference_unit = 'mg/dL' and lab_type_name='standard') OR\n",
    "        (lab_name='chloride' and reference_unit = 'mmol/L' and lab_type_name='standard') OR\n",
    "        (lab_name='potassium' and reference_unit = 'mmol/L' and lab_type_name='standard') OR\n",
    "        (lab_name='sodium' and reference_unit = 'mmol/L' and lab_type_name='standard') OR\n",
    "        (lab_name='glucose_serum' and reference_unit = 'mg/dL' and lab_type_name='standard') OR\n",
    "        (lab_name='hemoglobin' and reference_unit = 'g/dL' and lab_type_name='standard') OR\n",
    "        (lab_name='platelet count' and reference_unit = 'K/uL' and lab_type_name='standard') OR\n",
    "        (lab_name='wbc' and reference_unit = 'K/uL' and lab_type_name='standard'))  \n",
    "        AND encounter_id IN (SELECT DISTINCT encounter_id FROM icu_data);\n",
    "''').df()\n",
    "\n",
    "labs=con.execute('''\n",
    "PIVOT labs\n",
    "ON lab_name\n",
    "USING first(lab_value)\n",
    "GROUP BY encounter_id,lab_order_dttm;\n",
    "''').df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "icu_data_agg=pd.merge(icu_data,labs, on=['encounter_id'], how='left')\n",
    "icu_data_agg=icu_data_agg[(icu_data_agg['lab_order_dttm'] >= icu_data_agg['min_in_dttm']) & (icu_data_agg['lab_order_dttm'] <= icu_data_agg['after_24hr'])].reset_index(drop=True)\n",
    "\n",
    "Lab_variables = [\n",
    "   'albumin', 'alkaline_phosphatase',\n",
    "       'ast', 'basophil', 'bilirubin_conjugated', 'bilirubin_total', 'calcium',\n",
    "       'chloride', 'glucose_serum', 'hemoglobin', 'lymphocyte', 'monocyte',\n",
    "       'neutrophil', 'platelet count', 'potassium', 'sodium', 'total_protein',\n",
    "       'wbc'\n",
    "]\n",
    "agg_dict = {var: ['min', 'max', 'mean'] for var in Lab_variables}\n",
    "\n",
    "icu_data_agg = icu_data_agg.groupby('encounter_id').agg(agg_dict).reset_index()\n",
    "\n",
    "icu_data_agg.columns = ['_'.join(col).strip() if col[1] else col[0] for col in icu_data_agg.columns.values]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "icu_data=pd.merge(icu_data,icu_data_agg, on=['encounter_id'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_col=['isfemale','age', 'min_bmi', 'max_bmi', 'avg_bmi',\n",
    "       'min_weight_kg', 'max_weight_kg', 'avg_weight_kg', 'min_pulse',\n",
    "       'max_pulse', 'avg_pulse', 'min_sbp', 'max_sbp', 'avg_sbp', 'min_dbp',\n",
    "       'max_dbp', 'avg_dbp', 'min_temp_c', 'max_temp_c', 'avg_temp_c',\n",
    "       'albumin_min', 'albumin_max', 'albumin_mean',\n",
    "       'alkaline_phosphatase_min', 'alkaline_phosphatase_max',\n",
    "       'alkaline_phosphatase_mean', 'ast_min', 'ast_max', 'ast_mean',\n",
    "       'basophil_min', 'basophil_max', 'basophil_mean',\n",
    "       'bilirubin_conjugated_min', 'bilirubin_conjugated_max',\n",
    "       'bilirubin_conjugated_mean', 'bilirubin_total_min',\n",
    "       'bilirubin_total_max', 'bilirubin_total_mean', 'calcium_min',\n",
    "       'calcium_max', 'calcium_mean', 'chloride_min', 'chloride_max',\n",
    "       'chloride_mean', 'glucose_serum_min', 'glucose_serum_max',\n",
    "       'glucose_serum_mean', 'hemoglobin_min', 'hemoglobin_max',\n",
    "       'hemoglobin_mean', 'lymphocyte_min', 'lymphocyte_max',\n",
    "       'lymphocyte_mean', 'monocyte_min', 'monocyte_max', 'monocyte_mean',\n",
    "       'neutrophil_min', 'neutrophil_max', 'neutrophil_mean',\n",
    "       'platelet count_min', 'platelet count_max', 'platelet count_mean',\n",
    "       'potassium_min', 'potassium_max', 'potassium_mean', 'sodium_min',\n",
    "       'sodium_max', 'sodium_mean', 'total_protein_min', 'total_protein_max',\n",
    "       'total_protein_mean', 'wbc_min', 'wbc_max', 'wbc_mean']\n",
    "\n",
    "\n",
    "model=lgb.Booster(model_file=f'{tables_location}/projects/Mortality_model/models/lgbm_model_20240425-112249.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### basic metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "      <th>SiteName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.957052</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.457457</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.936330</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ROC AUC</td>\n",
       "      <td>0.951201</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brier Score Loss</td>\n",
       "      <td>0.032766</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Metric     Value SiteName\n",
       "0          Accuracy  0.957052     RUSH\n",
       "1            Recall  0.457457     RUSH\n",
       "2         Precision  0.936330     RUSH\n",
       "3           ROC AUC  0.951201     RUSH\n",
       "4  Brier Score Loss  0.032766     RUSH"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test=icu_data[model_col]\n",
    "y_test=icu_data['isdeathdispo']\n",
    "\n",
    "y_pred_proba = model.predict(X_test)\n",
    "icu_data['pred_proba'] = y_pred_proba\n",
    "# Calculate metrics at default threshold (0.5)\n",
    "\n",
    "accuracy = accuracy_score(y_test, (y_pred_proba >= 0.5).astype(int))\n",
    "recall = recall_score(y_test, (y_pred_proba >= 0.5).astype(int))\n",
    "precision = precision_score(y_test, (y_pred_proba >= 0.5).astype(int))\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "brier_score = brier_score_loss(y_test, y_pred_proba)\n",
    "\n",
    "\n",
    "results_Metric = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Recall', 'Precision', 'ROC AUC', 'Brier Score Loss'],\n",
    "    'Value': [accuracy, recall, precision, roc_auc, brier_score],\n",
    "    'SiteName': [f'{site_name}'] * 5\n",
    "})\n",
    "\n",
    "results_Metric.to_csv(f'output/result_metrics_{site_name}.csv',index=False)\n",
    "results_Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### probablity table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_label</th>\n",
       "      <th>site_proba</th>\n",
       "      <th>Site_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.022355</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.012466</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.023490</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.005411</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.025744</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   site_label   site_proba Site_name\n",
       "0            0    0.022355      RUSH\n",
       "1            0    0.012466      RUSH\n",
       "2            0    0.023490      RUSH\n",
       "3            0    0.005411      RUSH\n",
       "4            0    0.025744      RUSH"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_df_lgbm = pd.DataFrame({'site_label ':y_test, 'site_proba': y_pred_proba,'Site_name':f\"{site_name}\" })\n",
    "prob_df_lgbm.to_csv(f'output/Model_probabilities_{site_name}.csv',index=False)\n",
    "prob_df_lgbm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model fairness test accross 'race', 'ethnicity', 'sex'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(data, true_col, pred_prob_col, subgroup_cols):\n",
    "    results = []\n",
    "    total_count = len(data)\n",
    "\n",
    "    for subgroup_col in subgroup_cols:\n",
    "       \n",
    "        filtered_data = data.dropna(subset=[subgroup_col])\n",
    "        \n",
    "        for group in filtered_data[subgroup_col].unique():\n",
    "            subgroup_data = filtered_data[filtered_data[subgroup_col] == group]\n",
    "            group_count = len(subgroup_data)\n",
    "            proportion = group_count / total_count\n",
    "\n",
    "            if np.unique(subgroup_data[true_col]).size > 1:  # Check if both classes are present\n",
    "                auc = roc_auc_score(subgroup_data[true_col], subgroup_data[pred_prob_col])\n",
    "                tn, fp, fn, tp = confusion_matrix(subgroup_data[true_col], (subgroup_data[pred_prob_col] > 0.5).astype(int)).ravel()\n",
    "                ppv = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "                result = {'Subgroup': subgroup_col, 'Group': group, 'AUC': auc, 'PPV': ppv, 'Group Count': group_count, 'Total Count': total_count, 'Proportion': proportion, 'site_name':f'{site_name}'}\n",
    "            else:\n",
    "                result = {'Subgroup': subgroup_col, 'Group': group, 'AUC': 'Not defined', 'PPV': 'Not applicable', 'Group Count': group_count, 'Total Count': total_count, 'Proportion': proportion, 'site_name':f'{site_name}'}\n",
    "            \n",
    "            results.append(result)\n",
    "    \n",
    "   \n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "result_df = calculate_metrics(icu_data, 'isdeathdispo', 'pred_proba', ['race', 'ethnicity', 'sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subgroup</th>\n",
       "      <th>Group</th>\n",
       "      <th>AUC</th>\n",
       "      <th>PPV</th>\n",
       "      <th>Group Count</th>\n",
       "      <th>Total Count</th>\n",
       "      <th>Proportion</th>\n",
       "      <th>site_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>race</td>\n",
       "      <td>White</td>\n",
       "      <td>0.950923</td>\n",
       "      <td>0.942708</td>\n",
       "      <td>5621</td>\n",
       "      <td>14599</td>\n",
       "      <td>0.385026</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>race</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.953310</td>\n",
       "      <td>0.932773</td>\n",
       "      <td>2731</td>\n",
       "      <td>14599</td>\n",
       "      <td>0.187068</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>race</td>\n",
       "      <td>Black</td>\n",
       "      <td>0.948128</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>5762</td>\n",
       "      <td>14599</td>\n",
       "      <td>0.394685</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>race</td>\n",
       "      <td>Asian</td>\n",
       "      <td>0.964165</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>485</td>\n",
       "      <td>14599</td>\n",
       "      <td>0.033221</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ethnicity</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>0.952603</td>\n",
       "      <td>0.937965</td>\n",
       "      <td>11621</td>\n",
       "      <td>14599</td>\n",
       "      <td>0.796013</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ethnicity</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>0.946807</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>2874</td>\n",
       "      <td>14599</td>\n",
       "      <td>0.196863</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ethnicity</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.899901</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>104</td>\n",
       "      <td>14599</td>\n",
       "      <td>0.007124</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sex</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.950824</td>\n",
       "      <td>0.932384</td>\n",
       "      <td>7779</td>\n",
       "      <td>14599</td>\n",
       "      <td>0.532845</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sex</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.951510</td>\n",
       "      <td>0.940711</td>\n",
       "      <td>6820</td>\n",
       "      <td>14599</td>\n",
       "      <td>0.467155</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Subgroup                   Group       AUC       PPV  Group Count  \\\n",
       "0       race                   White  0.950923  0.942708         5621   \n",
       "1       race                  Others  0.953310  0.932773         2731   \n",
       "2       race                   Black  0.948128  0.930000         5762   \n",
       "3       race                   Asian  0.964165  0.956522          485   \n",
       "4  ethnicity  Not Hispanic or Latino  0.952603  0.937965        11621   \n",
       "5  ethnicity      Hispanic or Latino  0.946807  0.926829         2874   \n",
       "6  ethnicity                  Others  0.899901  1.000000          104   \n",
       "7        sex                    Male  0.950824  0.932384         7779   \n",
       "8        sex                  Female  0.951510  0.940711         6820   \n",
       "\n",
       "   Total Count  Proportion site_name  \n",
       "0        14599    0.385026      RUSH  \n",
       "1        14599    0.187068      RUSH  \n",
       "2        14599    0.394685      RUSH  \n",
       "3        14599    0.033221      RUSH  \n",
       "4        14599    0.796013      RUSH  \n",
       "5        14599    0.196863      RUSH  \n",
       "6        14599    0.007124      RUSH  \n",
       "7        14599    0.532845      RUSH  \n",
       "8        14599    0.467155      RUSH  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.to_csv(f'output/fairness_test_{site_name}.csv',index=False)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### thrshold check at site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_percentile(target_var, pred_proba):\n",
    "    #thr_list = [0.99,0.97, 0.95,0.90,0.80,0.70,0.60,0.50,0.40,0.30,0.20,0.10]\n",
    "    thr_list = np.arange(1, 0, -0.01)\n",
    "    col = ['N Percentile', 'Thr Value','TN','FP','FN','TP','Sensitivity','Specificity','PPV', 'NPV' ,'Recall','Accuracy','site_name']\n",
    "    result = pd.DataFrame(columns = col)\n",
    "    i = 0\n",
    "    \n",
    "    for thr in thr_list: \n",
    "        prob = pd.DataFrame()\n",
    "        prob['target_var'] = target_var\n",
    "        prob['pred_proba'] = pred_proba\n",
    "\n",
    "        thr_value = prob['pred_proba'].quantile(thr)\n",
    "        prob['pred_proba_bin'] = np.where(prob['pred_proba'] >= thr_value, 1, 0)\n",
    "        tn,fp,fn,tp = confusion_matrix(prob['target_var'], prob['pred_proba_bin']).ravel()\n",
    "\n",
    "        sensitivity = tp/(tp+fn)\n",
    "        specificity = tn/(tn+fp)\n",
    "        ppv = tp/(tp+fp)\n",
    "        npv = tn/(tn+fn)\n",
    "        recall = tp/(tp+fn)\n",
    "        acc = (tp+tn)/(tp+fn+tn+fp)\n",
    "        n_prec = 'Top '+ str(np.round((1 - thr) * 100,0))+ \"%\"\n",
    "        result.loc[i] = [n_prec,thr_value,tn,fp,fn,tp,sensitivity,specificity ,ppv,npv, recall, acc,f'{site_name}']\n",
    "        i+=1\n",
    "    return result\n",
    "topn=top_n_percentile(y_test,y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N Percentile</th>\n",
       "      <th>Thr Value</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TP</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>PPV</th>\n",
       "      <th>NPV</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>site_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top 0.0%</td>\n",
       "      <td>0.989530</td>\n",
       "      <td>13506</td>\n",
       "      <td>0</td>\n",
       "      <td>1092</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.925195</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>0.925200</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Top 1.0%</td>\n",
       "      <td>0.777559</td>\n",
       "      <td>13504</td>\n",
       "      <td>2</td>\n",
       "      <td>949</td>\n",
       "      <td>144</td>\n",
       "      <td>0.131747</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>0.986301</td>\n",
       "      <td>0.934339</td>\n",
       "      <td>0.131747</td>\n",
       "      <td>0.934859</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Top 2.0%</td>\n",
       "      <td>0.657780</td>\n",
       "      <td>13494</td>\n",
       "      <td>12</td>\n",
       "      <td>813</td>\n",
       "      <td>280</td>\n",
       "      <td>0.256176</td>\n",
       "      <td>0.999112</td>\n",
       "      <td>0.958904</td>\n",
       "      <td>0.943175</td>\n",
       "      <td>0.256176</td>\n",
       "      <td>0.943489</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Top 3.0%</td>\n",
       "      <td>0.556040</td>\n",
       "      <td>13485</td>\n",
       "      <td>21</td>\n",
       "      <td>676</td>\n",
       "      <td>417</td>\n",
       "      <td>0.381519</td>\n",
       "      <td>0.998445</td>\n",
       "      <td>0.952055</td>\n",
       "      <td>0.952263</td>\n",
       "      <td>0.381519</td>\n",
       "      <td>0.952257</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Top 4.0%</td>\n",
       "      <td>0.464467</td>\n",
       "      <td>13465</td>\n",
       "      <td>41</td>\n",
       "      <td>550</td>\n",
       "      <td>543</td>\n",
       "      <td>0.496798</td>\n",
       "      <td>0.996964</td>\n",
       "      <td>0.929795</td>\n",
       "      <td>0.960756</td>\n",
       "      <td>0.496798</td>\n",
       "      <td>0.959518</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Top 5.0%</td>\n",
       "      <td>0.378441</td>\n",
       "      <td>13429</td>\n",
       "      <td>77</td>\n",
       "      <td>440</td>\n",
       "      <td>653</td>\n",
       "      <td>0.597438</td>\n",
       "      <td>0.994299</td>\n",
       "      <td>0.894521</td>\n",
       "      <td>0.968275</td>\n",
       "      <td>0.597438</td>\n",
       "      <td>0.964587</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  N Percentile  Thr Value     TN  FP    FN   TP  Sensitivity  Specificity  \\\n",
       "0     Top 0.0%   0.989530  13506   0  1092    1     0.000915     1.000000   \n",
       "1     Top 1.0%   0.777559  13504   2   949  144     0.131747     0.999852   \n",
       "2     Top 2.0%   0.657780  13494  12   813  280     0.256176     0.999112   \n",
       "3     Top 3.0%   0.556040  13485  21   676  417     0.381519     0.998445   \n",
       "4     Top 4.0%   0.464467  13465  41   550  543     0.496798     0.996964   \n",
       "5     Top 5.0%   0.378441  13429  77   440  653     0.597438     0.994299   \n",
       "\n",
       "        PPV       NPV    Recall  Accuracy site_name  \n",
       "0  1.000000  0.925195  0.000915  0.925200      RUSH  \n",
       "1  0.986301  0.934339  0.131747  0.934859      RUSH  \n",
       "2  0.958904  0.943175  0.256176  0.943489      RUSH  \n",
       "3  0.952055  0.952263  0.381519  0.952257      RUSH  \n",
       "4  0.929795  0.960756  0.496798  0.959518      RUSH  \n",
       "5  0.894521  0.968275  0.597438  0.964587      RUSH  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topn.to_csv(f'output/Top_N_percentile_PPV_{site_name}.csv',index=False)\n",
    "topn.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FineTune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 555, number of negative: 6744\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004064 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12983\n",
      "[LightGBM] [Info] Number of data points in the train set: 7299, number of used features: 74\n",
      "             Metric     Value SiteName FineTune\n",
      "0          Accuracy  0.956438     RUSH      Yes\n",
      "1            Recall  0.457249     RUSH      Yes\n",
      "2         Precision  0.904412     RUSH      Yes\n",
      "3           ROC AUC  0.953286     RUSH      Yes\n",
      "4  Brier Score Loss  0.032374     RUSH      Yes\n"
     ]
    }
   ],
   "source": [
    "if finetune:\n",
    "    train_data, test_data = train_test_split(icu_data, test_size=0.5, random_state=42)\n",
    "    X_train=train_data[model_col]\n",
    "    y_train=train_data['isdeathdispo']\n",
    "\n",
    "    #test\n",
    "    X_test=test_data[model_col]\n",
    "    y_test=test_data['isdeathdispo']\n",
    "\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "\n",
    "    params = {\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"binary_logloss\",\n",
    "        \"num_leaves\": 31,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"max_depth\":-1}\n",
    "    gbm = lgb.train(params, lgb_train, num_boost_round=10, init_model=f'{tables_location}/projects/Mortality_model/models/lgbm_model_20240425-112249.txt')\n",
    "\n",
    "    y_pred_proba_ft = gbm.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, (y_pred_proba_ft >= 0.5).astype(int))\n",
    "    recall = recall_score(y_test, (y_pred_proba_ft >= 0.5).astype(int))\n",
    "    precision = precision_score(y_test, (y_pred_proba_ft >= 0.5).astype(int))\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba_ft)\n",
    "    brier_score = brier_score_loss(y_test, y_pred_proba_ft)\n",
    "\n",
    "\n",
    "    results_Metric = pd.DataFrame({\n",
    "        'Metric': ['Accuracy', 'Recall', 'Precision', 'ROC AUC', 'Brier Score Loss'],\n",
    "        'Value': [accuracy, recall, precision, roc_auc, brier_score],\n",
    "        'SiteName': [f'{site_name}'] * 5,\n",
    "        'FineTune': ['Yes'] * 5,\n",
    "    })\n",
    "    results_Metric.to_csv(f'output/result_metrics_{site_name}_ft.csv',index=False)\n",
    "\n",
    "\n",
    "    model_filename = f\"output/lgbm_model_{site_name}_ft.txt\"\n",
    "\n",
    "    # Save the model using LightGBM's built-in function\n",
    "    model.save_model(model_filename)\n",
    "\n",
    "    print(results_Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
