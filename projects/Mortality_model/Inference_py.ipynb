{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#!{sys.executable} -m pip3 install pandas numpy scikit-learn lightgbm matplotlib duckdb pyarrow\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import warnings\n",
    "import duckdb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (confusion_matrix, auc, roc_curve, accuracy_score, \n",
    "                             precision_score, recall_score, f1_score, \n",
    "                             precision_recall_curve, roc_auc_score, brier_score_loss)\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "# install pyarrow to work with parquet files\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "#pd.set_option('display.max_rows', None)\n",
    "random.seed(37)\n",
    "np.random.seed(37)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "con = duckdb.connect(database=\":memory:\")\n",
    "\n",
    "print(\"Setup complete\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control panel- User Input required\n",
    "\n",
    "Update root location, input filetype, site_name and confirm that race/ethnicity mapping correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter the location for your CLIF-1.0 directory\n",
    "root_location = 'C:/Users/vchaudha/OneDrive - rush.edu/CLIF-1.0-main'\n",
    "# either parquet or csv only\n",
    "filetype = 'csv'\n",
    "site_name='RUSH'\n",
    "\n",
    "race_map = {\n",
    "    'White': 'White',\n",
    "    'Black or African American': 'Black',\n",
    "    'Asian': 'Asian',\n",
    "    'Other': 'Others',\n",
    "    'Unknown': 'Others',\n",
    "    'Did Not Encounter': 'Others',\n",
    "    'Refusal': 'Others',\n",
    "    'American Indian or Alaska Native': 'Others',\n",
    "    'Native Hawaiian or Other Pacific Islander': 'Others',\n",
    "    np.nan: 'Others'\n",
    "}\n",
    "\n",
    "ethnicity_map = {\n",
    "    'Not Hispanic or Latino': 'Not Hispanic or Latino',\n",
    "    'Hispanic or Latino': 'Hispanic or Latino',\n",
    "    'Did Not Encounter': 'Others',\n",
    "    'Refusal': 'Others',\n",
    "    '*Unspecified': 'Others',\n",
    "    np.nan: 'Others'\n",
    "}\n",
    "\n",
    "finetune=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adt_filepath = f\"{root_location}/rclif/clif_adt.{filetype}\"\n",
    "encounter_filepath = f\"{root_location}/rclif/clif_encounter_demographics_dispo_clean.{filetype}\"\n",
    "limited_filepath = f\"{root_location}/rclif/clif_limited_identifiers.{filetype}\"\n",
    "demog_filepath = f\"{root_location}/rclif/clif_patient_demographics.{filetype}\"\n",
    "vitals_filepath = f\"{root_location}/rclif/clif_vitals_clean.{filetype}\"\n",
    "labs_filepath = f\"{root_location}/rclif/clif_labs_clean.{filetype}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filepath, filetype):\n",
    "    \"\"\"\n",
    "    Read data from file based on file type.\n",
    "    Parameters:\n",
    "        filepath (str): Path to the file.\n",
    "        filetype (str): Type of the file ('csv' or 'parquet').\n",
    "    Returns:\n",
    "        DataFrame: DataFrame containing the data.\n",
    "    \"\"\"\n",
    "    if filetype == 'csv':\n",
    "        return pd.read_csv(filepath)\n",
    "    elif filetype == 'parquet':\n",
    "        table = pq.read_table(filepath)\n",
    "        return table.to_pandas()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Please provide either 'csv' or 'parquet'.\")\n",
    "    \n",
    "\n",
    "def standardize_datetime(df):\n",
    "    \"\"\"\n",
    "    Ensure that all *_dttm variables are in the correct format.\n",
    "    Convert all datetime columns to a specific precision and remove timezone\n",
    "    Parameters:\n",
    "        DataFrame: DataFrame containing the data.\n",
    "    Returns:\n",
    "        DataFrame: DataFrame containing the data.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            # Here converting to 'datetime64[ns]' for uniformity and removing timezone with 'tz_convert(None)'\n",
    "            df[col] = df[col].dt.tz_convert(None) if df[col].dt.tz is not None else df[col]\n",
    "            # If you need to standardize to UTC and keep the timezone:\n",
    "            # df[col] = df[col].dt.tz_localize('UTC') if df[col].dt.tz is None else df[col].dt.tz_convert('UTC')\n",
    "    return df\n",
    "\n",
    "def get_sql_import(filetype):\n",
    "    if filetype == 'parquet':\n",
    "        return 'read_parquet'\n",
    "    if filetype == 'csv':\n",
    "        return 'read_csv_auto'\n",
    "\n",
    "sql_import = get_sql_import(filetype=filetype)\n",
    "\n",
    "# create output directory\n",
    "output_directory = os.path.join(os.getcwd(), 'output')\n",
    "os.makedirs(output_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = read_data(adt_filepath, filetype)\n",
    "encounter = read_data(encounter_filepath, filetype)\n",
    "limited = read_data(limited_filepath, filetype)\n",
    "demog = read_data(demog_filepath, filetype)\n",
    "\n",
    "# Apply the standardization function to each DataFrame\n",
    "location = standardize_datetime(location)\n",
    "encounter = standardize_datetime(encounter)\n",
    "limited = standardize_datetime(limited)\n",
    "demog = standardize_datetime(demog)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICU close to Admission\n",
    "\n",
    "1. Check ICU location_category between admission_dttmtime and 48hr stop from admission\n",
    "2. Check ICU stay at least 24 hr (for ICU - OR - ICU including OR in ICU stay 24hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "join=pd.merge(location[['encounter_id','location_category','in_dttm','out_dttm']],\\\n",
    "              limited[['encounter_id','admission_dttm']], on=['encounter_id'], how='left')\n",
    "\n",
    "icu_data=pd.merge(join,\\\n",
    "                  encounter[['encounter_id','age_at_admission','disposition']], on=['encounter_id'], how='left')\n",
    "\n",
    "\n",
    "icu_data['in_dttm'] = pd.to_datetime(icu_data['in_dttm'])\n",
    "icu_data['admission_dttm'] = pd.to_datetime(icu_data['admission_dttm'])\n",
    "icu_data['out_dttm'] = pd.to_datetime(icu_data['out_dttm'])\n",
    "#icu_data['age_at_admission'] = icu_data['age_at_admission'].astype(int)\n",
    "\n",
    "icu_48hr_check = icu_data[\n",
    "    (icu_data['location_category'] == 'ICU') &\n",
    "    (icu_data['in_dttm'] >= icu_data['admission_dttm']) &\n",
    "    (icu_data['in_dttm'] <= icu_data['admission_dttm'] + pd.Timedelta(hours=48)) &\n",
    "    (icu_data['in_dttm'].dt.year >= 2020) & (icu_data['in_dttm'].dt.year <= 2022) & \n",
    "    (icu_data['age_at_admission'] >= 18) & (icu_data['age_at_admission'].notna())\n",
    "]['encounter_id'].unique()\n",
    "\n",
    "icu_data=icu_data[icu_data['encounter_id'].isin(icu_48hr_check) & (icu_data['in_dttm'] <= icu_data['admission_dttm'] + pd.Timedelta(hours=72))].reset_index(drop=True)\n",
    "\n",
    "icu_data = icu_data.sort_values(by=['in_dttm']).reset_index(drop=True)\n",
    "\n",
    "icu_data[\"RANK\"]=icu_data.sort_values(by=['in_dttm'], ascending=True).groupby(\"encounter_id\")[\"in_dttm\"].rank(method=\"first\", ascending=True).astype(int)\n",
    "\n",
    "\n",
    "min_icu=icu_data[icu_data['location_category'] == 'ICU'].groupby('encounter_id')['RANK'].min()\n",
    "icu_data=pd.merge(icu_data, pd.DataFrame(zip(min_icu.index, min_icu.values), columns=['encounter_id', 'min_icu']), on='encounter_id', how='left')\n",
    "icu_data=icu_data[icu_data['RANK']>=icu_data['min_icu']].reset_index(drop=True)\n",
    "\n",
    "icu_data.loc[icu_data['location_category'] == 'OR', 'location_category'] = 'ICU'\n",
    "\n",
    "icu_data['group_id'] = (icu_data.groupby('encounter_id')['location_category'].shift() != icu_data['location_category']).astype(int)\n",
    "icu_data['group_id'] = icu_data.sort_values(by=['in_dttm'], ascending=True).groupby('encounter_id')['group_id'].cumsum()\n",
    "\n",
    "\n",
    "icu_data = icu_data.sort_values(by=['in_dttm'], ascending=True).groupby(['encounter_id', 'location_category', 'group_id']).agg(\n",
    "    min_in_dttm=('in_dttm', 'min'),\n",
    "    max_out_dttm=('out_dttm', 'max'),\n",
    "    admission_dttm=('admission_dttm', 'first'),\n",
    "    age=('age_at_admission', 'first'),\n",
    "    dispo=('disposition', 'first')\n",
    ").reset_index()\n",
    "\n",
    "min_icu=icu_data[icu_data['location_category'] == 'ICU'].groupby('encounter_id')['group_id'].min()\n",
    "icu_data=pd.merge(icu_data, pd.DataFrame(zip(min_icu.index, min_icu.values), columns=['encounter_id', 'min_icu']), on='encounter_id', how='left')\n",
    "\n",
    "icu_data=icu_data[(icu_data['min_icu']==icu_data['group_id']) &\n",
    "         (icu_data['max_out_dttm']-icu_data['min_in_dttm'] >= pd.Timedelta(hours=24))\n",
    "         ].reset_index(drop=True)\n",
    "\n",
    "\n",
    "icu_data['after_24hr']=icu_data['min_in_dttm'] + pd.Timedelta(hours=24)\n",
    "\n",
    "icu_data=icu_data[['encounter_id','min_in_dttm','after_24hr','age','dispo']]\n",
    "\n",
    "icu_data=pd.merge(icu_data,\\\n",
    "                  demog, on=['encounter_id'], how='left')[['encounter_id','min_in_dttm','after_24hr','age','dispo','sex','ethnicity','race']]\n",
    "icu_data=icu_data[~icu_data['sex'].isna()].reset_index(drop=True)\n",
    "icu_data['isfemale']=(icu_data['sex'].str.lower() == 'female').astype(int)\n",
    "icu_data['isdeathdispo'] = (icu_data['dispo'].str.contains('dead|expired', case=False, regex=True)).astype(int)\n",
    "\n",
    "icu_data['ethnicity'] = icu_data['ethnicity'].map(ethnicity_map)\n",
    "icu_data['race'] = icu_data['race'].map(race_map)\n",
    "\n",
    "\n",
    "del location,encounter,limited,demog"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3216f13517c848efbc34404a05b00722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vitals = con.execute(f'''\n",
    "    SELECT \n",
    "        encounter_id,\n",
    "        CAST(recorded_dttm AS datetime) AS recorded_dttm,\n",
    "        CAST(vital_value AS float) AS vital_value,\n",
    "        vital_category \n",
    "    FROM \n",
    "        {sql_import}('{vitals_filepath}')\n",
    "    WHERE \n",
    "        vital_category IN ('weight_kg', 'pulse', 'sbp', 'dbp', 'temp_c','height_inches') \n",
    "        AND encounter_id IN (SELECT DISTINCT encounter_id FROM icu_data);\n",
    "''').df()\n",
    "\n",
    "vitals=con.execute('''\n",
    "PIVOT vitals\n",
    "ON vital_category\n",
    "USING first(vital_value)\n",
    "GROUP BY encounter_id,recorded_dttm;\n",
    "''').df()\n",
    "\n",
    "vitals['height_meters'] = vitals['height_inches'] * 0.0254\n",
    "\n",
    "# Calculate BMI\n",
    "vitals['bmi'] = vitals['weight_kg'] / (vitals['height_meters'] ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "icu_data_agg=pd.merge(icu_data,vitals, on=['encounter_id'], how='left')\n",
    "icu_data_agg=icu_data_agg[(icu_data_agg['recorded_dttm'] >= icu_data_agg['min_in_dttm']) & (icu_data_agg['recorded_dttm'] <= icu_data_agg['after_24hr'])].reset_index(drop=True)\n",
    "\n",
    "icu_data_agg = icu_data_agg.groupby(['encounter_id']).agg(\n",
    "    min_bmi=('bmi', 'min'),\n",
    "    max_bmi=('bmi', 'max'),\n",
    "    avg_bmi=('bmi', 'mean'),\n",
    "    min_weight_kg=('weight_kg', 'min'),\n",
    "    max_weight_kg=('weight_kg', 'max'),\n",
    "    avg_weight_kg=('weight_kg', 'mean'),\n",
    "    min_pulse=('pulse', 'min'),\n",
    "    max_pulse=('pulse', 'max'),\n",
    "    avg_pulse=('pulse', 'mean'),\n",
    "    min_sbp=('sbp', 'min'),\n",
    "    max_sbp=('sbp', 'max'),\n",
    "    avg_sbp=('sbp', 'mean'),\n",
    "    min_dbp=('dbp', 'min'),\n",
    "    max_dbp=('dbp', 'max'),\n",
    "    avg_dbp=('dbp', 'mean'),\n",
    "    min_temp_c=('temp_c', 'min'),\n",
    "    max_temp_c=('temp_c', 'max'),\n",
    "    avg_temp_c=('temp_c', 'mean'),\n",
    ").reset_index()\n",
    "\n",
    "icu_data=pd.merge(icu_data,icu_data_agg, on=['encounter_id'], how='left')\n",
    "\n",
    "del vitals,icu_data_agg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8778f337e71450187e8ceb5dae8695a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labs = con.execute(f'''\n",
    "    SELECT \n",
    "        encounter_id,\n",
    "        CAST(lab_order_dttm AS datetime) AS lab_order_dttm,\n",
    "        TRY_CAST(lab_value AS float) AS lab_value,\n",
    "        lab_category\n",
    "    FROM \n",
    "         {sql_import}('{labs_filepath}')\n",
    "    WHERE \n",
    "         ((lab_category='monocyte' and reference_unit = '%' and lab_type_name='standard') OR\n",
    "        (lab_category='lymphocyte' and reference_unit = '%' and lab_type_name='standard') OR\n",
    "        (lab_category='basophil' and reference_unit = '%' and lab_type_name='standard') OR\n",
    "        (lab_category='neutrophil' and reference_unit = '%' and lab_type_name='standard') OR\n",
    "        (lab_category='albumin' and reference_unit = 'g/dL' and lab_type_name='standard') OR\n",
    "        (lab_category='ast' and reference_unit = 'U/L' and lab_type_name='standard') OR\n",
    "        (lab_category='total_protein' and reference_unit = 'g/dL' and lab_type_name='standard') OR\n",
    "        (lab_category='alkaline_phosphatase' and reference_unit = 'U/L' and lab_type_name='standard') OR\n",
    "        (lab_category='bilirubin_total' and reference_unit = 'mg/dL' and lab_type_name='standard') OR\n",
    "        (lab_category='bilirubin_conjugated' and reference_unit = 'mg/dL' and lab_type_name='standard') OR\n",
    "        (lab_category='calcium' and reference_unit = 'mg/dL' and lab_type_name='standard') OR\n",
    "        (lab_category='chloride' and reference_unit = 'mmol/L' and lab_type_name='standard') OR\n",
    "        (lab_category='potassium' and reference_unit = 'mmol/L' and lab_type_name='standard') OR\n",
    "        (lab_category='sodium' and reference_unit = 'mmol/L' and lab_type_name='standard') OR\n",
    "        (lab_category='glucose_serum' and reference_unit = 'mg/dL' and lab_type_name='standard') OR\n",
    "        (lab_category='hemoglobin' and reference_unit = 'g/dL' and lab_type_name='standard') OR\n",
    "        (lab_category='platelet count' and reference_unit = 'K/uL' and lab_type_name='standard') OR\n",
    "        (lab_category='wbc' and reference_unit = 'K/uL' and lab_type_name='standard'))  \n",
    "        AND encounter_id IN (SELECT DISTINCT encounter_id FROM icu_data);\n",
    "''').df()\n",
    "\n",
    "labs=con.execute('''\n",
    "PIVOT labs\n",
    "ON lab_category\n",
    "USING first(lab_value)\n",
    "GROUP BY encounter_id,lab_order_dttm;\n",
    "''').df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "icu_data_agg=pd.merge(icu_data,labs, on=['encounter_id'], how='left')\n",
    "icu_data_agg=icu_data_agg[(icu_data_agg['lab_order_dttm'] >= icu_data_agg['min_in_dttm']) & (icu_data_agg['lab_order_dttm'] <= icu_data_agg['after_24hr'])].reset_index(drop=True)\n",
    "\n",
    "\n",
    "Lab_variables = [\n",
    "   'albumin', 'alkaline_phosphatase',\n",
    "       'ast', 'basophil', 'bilirubin_conjugated', 'bilirubin_total', 'calcium',\n",
    "       'chloride', 'hemoglobin', 'lymphocyte', 'monocyte', 'glucose_serum', \n",
    "       'neutrophil', 'potassium', 'sodium', 'total_protein','platelet count', \n",
    "       'wbc'\n",
    "]\n",
    "agg_dict = {var: ['min', 'max', 'mean'] for var in Lab_variables}\n",
    "\n",
    "icu_data_agg = icu_data_agg.groupby('encounter_id').agg(agg_dict).reset_index()\n",
    "\n",
    "icu_data_agg.columns = ['_'.join(col).strip() if col[1] else col[0] for col in icu_data_agg.columns.values]\n",
    "\n",
    "icu_data=pd.merge(icu_data,icu_data_agg, on=['encounter_id'], how='left')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_col=['isfemale','age', 'min_bmi', 'max_bmi', 'avg_bmi',\n",
    "       'min_weight_kg', 'max_weight_kg', 'avg_weight_kg', 'min_pulse',\n",
    "       'max_pulse', 'avg_pulse', 'min_sbp', 'max_sbp', 'avg_sbp', 'min_dbp',\n",
    "       'max_dbp', 'avg_dbp', 'min_temp_c', 'max_temp_c', 'avg_temp_c',\n",
    "       'albumin_min', 'albumin_max', 'albumin_mean',\n",
    "       'alkaline_phosphatase_min', 'alkaline_phosphatase_max',\n",
    "       'alkaline_phosphatase_mean', 'ast_min', 'ast_max', 'ast_mean',\n",
    "       'basophil_min', 'basophil_max', 'basophil_mean',\n",
    "       'bilirubin_conjugated_min', 'bilirubin_conjugated_max',\n",
    "       'bilirubin_conjugated_mean', 'bilirubin_total_min',\n",
    "       'bilirubin_total_max', 'bilirubin_total_mean', 'calcium_min',\n",
    "       'calcium_max', 'calcium_mean', 'chloride_min', 'chloride_max',\n",
    "       'chloride_mean', 'glucose_serum_min', 'glucose_serum_max',\n",
    "       'glucose_serum_mean', 'hemoglobin_min', 'hemoglobin_max',\n",
    "       'hemoglobin_mean', 'lymphocyte_min', 'lymphocyte_max',\n",
    "       'lymphocyte_mean', 'monocyte_min', 'monocyte_max', 'monocyte_mean',\n",
    "       'neutrophil_min', 'neutrophil_max', 'neutrophil_mean',\n",
    "       'platelet count_min', 'platelet count_max', 'platelet count_mean',\n",
    "       'potassium_min', 'potassium_max', 'potassium_mean', 'sodium_min',\n",
    "       'sodium_max', 'sodium_mean', 'total_protein_min', 'total_protein_max',\n",
    "       'total_protein_mean', 'wbc_min', 'wbc_max', 'wbc_mean']\n",
    "\n",
    "model=lgb.Booster(model_file=f'{root_location}/projects/Mortality_model/models/lgbm_model_20240529-110556.txt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### basic metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "      <th>SiteName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.954975</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.425435</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.941296</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ROC AUC</td>\n",
       "      <td>0.949366</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brier Score Loss</td>\n",
       "      <td>0.034548</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Metric     Value SiteName\n",
       "0          Accuracy  0.954975     RUSH\n",
       "1            Recall  0.425435     RUSH\n",
       "2         Precision  0.941296     RUSH\n",
       "3           ROC AUC  0.949366     RUSH\n",
       "4  Brier Score Loss  0.034548     RUSH"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test=icu_data[model_col]\n",
    "y_test=icu_data['isdeathdispo']\n",
    "\n",
    "y_pred_proba = model.predict(X_test)\n",
    "icu_data['pred_proba'] = y_pred_proba\n",
    "# Calculate metrics at default threshold (0.5)\n",
    "\n",
    "accuracy = accuracy_score(y_test, (y_pred_proba >= 0.5).astype(int))\n",
    "recall = recall_score(y_test, (y_pred_proba >= 0.5).astype(int))\n",
    "precision = precision_score(y_test, (y_pred_proba >= 0.5).astype(int))\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "brier_score = brier_score_loss(y_test, y_pred_proba)\n",
    "\n",
    "\n",
    "results_Metric = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Recall', 'Precision', 'ROC AUC', 'Brier Score Loss'],\n",
    "    'Value': [accuracy, recall, precision, roc_auc, brier_score],\n",
    "    'SiteName': [f'{site_name}'] * 5\n",
    "})\n",
    "\n",
    "results_Metric.to_csv(f'{output_directory}/result_metrics_{site_name}.csv',index=False)\n",
    "results_Metric"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### probablity table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_label</th>\n",
       "      <th>site_proba</th>\n",
       "      <th>Site_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.021630</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.015260</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.017889</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.005896</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.013865</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   site_label   site_proba Site_name\n",
       "0            0    0.021630      RUSH\n",
       "1            0    0.015260      RUSH\n",
       "2            0    0.017889      RUSH\n",
       "3            0    0.005896      RUSH\n",
       "4            0    0.013865      RUSH"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_df_lgbm = pd.DataFrame({'site_label ':y_test, 'site_proba': y_pred_proba,'Site_name':f\"{site_name}\" })\n",
    "prob_df_lgbm.to_csv(f'{output_directory}/Model_probabilities_{site_name}.csv',index=False)\n",
    "prob_df_lgbm.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model fairness test accross 'race', 'ethnicity', 'sex'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(data, true_col, pred_prob_col, subgroup_cols):\n",
    "    results = []\n",
    "    total_count = len(data)\n",
    "\n",
    "    for subgroup_col in subgroup_cols:\n",
    "       \n",
    "        filtered_data = data.dropna(subset=[subgroup_col])\n",
    "        \n",
    "        for group in filtered_data[subgroup_col].unique():\n",
    "            subgroup_data = filtered_data[filtered_data[subgroup_col] == group]\n",
    "            group_count = len(subgroup_data)\n",
    "            proportion = group_count / total_count\n",
    "\n",
    "            if np.unique(subgroup_data[true_col]).size > 1:  # Check if both classes are present\n",
    "                auc = roc_auc_score(subgroup_data[true_col], subgroup_data[pred_prob_col])\n",
    "                tn, fp, fn, tp = confusion_matrix(subgroup_data[true_col], (subgroup_data[pred_prob_col] > 0.5).astype(int)).ravel()\n",
    "                ppv = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "                result = {'Subgroup': subgroup_col, 'Group': group, 'AUC': auc, 'PPV': ppv, 'Group Count': group_count, 'Total Count': total_count, 'Proportion': proportion, 'site_name':f'{site_name}'}\n",
    "            else:\n",
    "                result = {'Subgroup': subgroup_col, 'Group': group, 'AUC': 'Not defined', 'PPV': 'Not applicable', 'Group Count': group_count, 'Total Count': total_count, 'Proportion': proportion, 'site_name':f'{site_name}'}\n",
    "            \n",
    "            results.append(result)\n",
    "    \n",
    "   \n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "result_df = calculate_metrics(icu_data, 'isdeathdispo', 'pred_proba', ['race', 'ethnicity', 'sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subgroup</th>\n",
       "      <th>Group</th>\n",
       "      <th>AUC</th>\n",
       "      <th>PPV</th>\n",
       "      <th>Group Count</th>\n",
       "      <th>Total Count</th>\n",
       "      <th>Proportion</th>\n",
       "      <th>site_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>race</td>\n",
       "      <td>White</td>\n",
       "      <td>0.942366</td>\n",
       "      <td>0.959064</td>\n",
       "      <td>5619</td>\n",
       "      <td>14592</td>\n",
       "      <td>0.385074</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>race</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.955985</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>2728</td>\n",
       "      <td>14592</td>\n",
       "      <td>0.186952</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>race</td>\n",
       "      <td>Black</td>\n",
       "      <td>0.951043</td>\n",
       "      <td>0.927536</td>\n",
       "      <td>5761</td>\n",
       "      <td>14592</td>\n",
       "      <td>0.394805</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>race</td>\n",
       "      <td>Asian</td>\n",
       "      <td>0.952960</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>484</td>\n",
       "      <td>14592</td>\n",
       "      <td>0.033169</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ethnicity</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>0.949394</td>\n",
       "      <td>0.943445</td>\n",
       "      <td>11617</td>\n",
       "      <td>14592</td>\n",
       "      <td>0.796121</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ethnicity</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>0.948557</td>\n",
       "      <td>0.936842</td>\n",
       "      <td>2875</td>\n",
       "      <td>14592</td>\n",
       "      <td>0.197026</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ethnicity</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.925156</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>100</td>\n",
       "      <td>14592</td>\n",
       "      <td>0.006853</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sex</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.946211</td>\n",
       "      <td>0.942748</td>\n",
       "      <td>7774</td>\n",
       "      <td>14592</td>\n",
       "      <td>0.532758</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sex</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.953008</td>\n",
       "      <td>0.939655</td>\n",
       "      <td>6818</td>\n",
       "      <td>14592</td>\n",
       "      <td>0.467242</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Subgroup                   Group       AUC       PPV  Group Count  \\\n",
       "0       race                   White  0.942366  0.959064         5619   \n",
       "1       race                  Others  0.955985  0.950000         2728   \n",
       "2       race                   Black  0.951043  0.927536         5761   \n",
       "3       race                   Asian  0.952960  0.875000          484   \n",
       "4  ethnicity  Not Hispanic or Latino  0.949394  0.943445        11617   \n",
       "5  ethnicity      Hispanic or Latino  0.948557  0.936842         2875   \n",
       "6  ethnicity                  Others  0.925156  0.900000          100   \n",
       "7        sex                    Male  0.946211  0.942748         7774   \n",
       "8        sex                  Female  0.953008  0.939655         6818   \n",
       "\n",
       "   Total Count  Proportion site_name  \n",
       "0        14592    0.385074      RUSH  \n",
       "1        14592    0.186952      RUSH  \n",
       "2        14592    0.394805      RUSH  \n",
       "3        14592    0.033169      RUSH  \n",
       "4        14592    0.796121      RUSH  \n",
       "5        14592    0.197026      RUSH  \n",
       "6        14592    0.006853      RUSH  \n",
       "7        14592    0.532758      RUSH  \n",
       "8        14592    0.467242      RUSH  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.to_csv(f'{output_directory}/fairness_test_{site_name}.csv',index=False)\n",
    "result_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### thrshold check at site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_percentile(target_var, pred_proba):\n",
    "    #thr_list = [0.99,0.97, 0.95,0.90,0.80,0.70,0.60,0.50,0.40,0.30,0.20,0.10]\n",
    "    thr_list = np.arange(1, 0, -0.01)\n",
    "    col = ['N Percentile', 'Thr Value','TN','FP','FN','TP','Sensitivity','Specificity','PPV', 'NPV' ,'Recall','Accuracy','site_name']\n",
    "    result = pd.DataFrame(columns = col)\n",
    "    i = 0\n",
    "    \n",
    "    for thr in thr_list: \n",
    "        prob = pd.DataFrame()\n",
    "        prob['target_var'] = target_var\n",
    "        prob['pred_proba'] = pred_proba\n",
    "\n",
    "        thr_value = prob['pred_proba'].quantile(thr)\n",
    "        prob['pred_proba_bin'] = np.where(prob['pred_proba'] >= thr_value, 1, 0)\n",
    "        tn,fp,fn,tp = confusion_matrix(prob['target_var'], prob['pred_proba_bin']).ravel()\n",
    "\n",
    "        sensitivity = tp/(tp+fn)\n",
    "        specificity = tn/(tn+fp)\n",
    "        ppv = tp/(tp+fp)\n",
    "        npv = tn/(tn+fn)\n",
    "        recall = tp/(tp+fn)\n",
    "        acc = (tp+tn)/(tp+fn+tn+fp)\n",
    "        n_prec = 'Top '+ str(np.round((1 - thr) * 100,0))+ \"%\"\n",
    "        result.loc[i] = [n_prec,thr_value,tn,fp,fn,tp,sensitivity,specificity ,ppv,npv, recall, acc,f'{site_name}']\n",
    "        i+=1\n",
    "    return result\n",
    "topn=top_n_percentile(y_test,y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N Percentile</th>\n",
       "      <th>Thr Value</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TP</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>PPV</th>\n",
       "      <th>NPV</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>site_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top 0.0%</td>\n",
       "      <td>0.991012</td>\n",
       "      <td>13499</td>\n",
       "      <td>0</td>\n",
       "      <td>1092</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.925159</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>0.925164</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Top 1.0%</td>\n",
       "      <td>0.775889</td>\n",
       "      <td>13494</td>\n",
       "      <td>5</td>\n",
       "      <td>952</td>\n",
       "      <td>141</td>\n",
       "      <td>0.129003</td>\n",
       "      <td>0.999630</td>\n",
       "      <td>0.965753</td>\n",
       "      <td>0.934099</td>\n",
       "      <td>0.129003</td>\n",
       "      <td>0.934416</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Top 2.0%</td>\n",
       "      <td>0.647020</td>\n",
       "      <td>13492</td>\n",
       "      <td>7</td>\n",
       "      <td>808</td>\n",
       "      <td>285</td>\n",
       "      <td>0.260750</td>\n",
       "      <td>0.999481</td>\n",
       "      <td>0.976027</td>\n",
       "      <td>0.943497</td>\n",
       "      <td>0.260750</td>\n",
       "      <td>0.944147</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Top 3.0%</td>\n",
       "      <td>0.532054</td>\n",
       "      <td>13477</td>\n",
       "      <td>22</td>\n",
       "      <td>677</td>\n",
       "      <td>416</td>\n",
       "      <td>0.380604</td>\n",
       "      <td>0.998370</td>\n",
       "      <td>0.949772</td>\n",
       "      <td>0.952169</td>\n",
       "      <td>0.380604</td>\n",
       "      <td>0.952097</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Top 4.0%</td>\n",
       "      <td>0.436592</td>\n",
       "      <td>13456</td>\n",
       "      <td>43</td>\n",
       "      <td>552</td>\n",
       "      <td>541</td>\n",
       "      <td>0.494968</td>\n",
       "      <td>0.996815</td>\n",
       "      <td>0.926370</td>\n",
       "      <td>0.960594</td>\n",
       "      <td>0.494968</td>\n",
       "      <td>0.959224</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Top 5.0%</td>\n",
       "      <td>0.347300</td>\n",
       "      <td>13409</td>\n",
       "      <td>90</td>\n",
       "      <td>453</td>\n",
       "      <td>640</td>\n",
       "      <td>0.585544</td>\n",
       "      <td>0.993333</td>\n",
       "      <td>0.876712</td>\n",
       "      <td>0.967321</td>\n",
       "      <td>0.585544</td>\n",
       "      <td>0.962788</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  N Percentile  Thr Value     TN  FP    FN   TP  Sensitivity  Specificity  \\\n",
       "0     Top 0.0%   0.991012  13499   0  1092    1     0.000915     1.000000   \n",
       "1     Top 1.0%   0.775889  13494   5   952  141     0.129003     0.999630   \n",
       "2     Top 2.0%   0.647020  13492   7   808  285     0.260750     0.999481   \n",
       "3     Top 3.0%   0.532054  13477  22   677  416     0.380604     0.998370   \n",
       "4     Top 4.0%   0.436592  13456  43   552  541     0.494968     0.996815   \n",
       "5     Top 5.0%   0.347300  13409  90   453  640     0.585544     0.993333   \n",
       "\n",
       "        PPV       NPV    Recall  Accuracy site_name  \n",
       "0  1.000000  0.925159  0.000915  0.925164      RUSH  \n",
       "1  0.965753  0.934099  0.129003  0.934416      RUSH  \n",
       "2  0.976027  0.943497  0.260750  0.944147      RUSH  \n",
       "3  0.949772  0.952169  0.380604  0.952097      RUSH  \n",
       "4  0.926370  0.960594  0.494968  0.959224      RUSH  \n",
       "5  0.876712  0.967321  0.585544  0.962788      RUSH  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topn.to_csv(f'{output_directory}/Top_N_percentile_PPV_{site_name}.csv',index=False)\n",
    "topn.head(6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FineTune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 547, number of negative: 6749\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003094 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12983\n",
      "[LightGBM] [Info] Number of data points in the train set: 7296, number of used features: 74\n",
      "             Metric     Value SiteName FineTune\n",
      "0          Accuracy  0.959293     RUSH      Yes\n",
      "1            Recall  0.478022     RUSH      Yes\n",
      "2         Precision  0.956044     RUSH      Yes\n",
      "3           ROC AUC  0.953316     RUSH      Yes\n",
      "4  Brier Score Loss  0.032632     RUSH      Yes\n"
     ]
    }
   ],
   "source": [
    "if finetune:\n",
    "    train_data, test_data = train_test_split(icu_data, test_size=0.5, random_state=42)\n",
    "    X_train=train_data[model_col]\n",
    "    y_train=train_data['isdeathdispo']\n",
    "\n",
    "    #test\n",
    "    X_test=test_data[model_col]\n",
    "    y_test=test_data['isdeathdispo']\n",
    "\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "\n",
    "    params = {\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"binary_logloss\",\n",
    "        \"num_leaves\": 31,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"max_depth\":-1}\n",
    "    gbm = lgb.train(params, lgb_train, num_boost_round=10, init_model=f'{root_location}/projects/Mortality_model/models/lgbm_model_20240529-110556.txt')\n",
    "\n",
    "    y_pred_proba_ft = gbm.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, (y_pred_proba_ft >= 0.5).astype(int))\n",
    "    recall = recall_score(y_test, (y_pred_proba_ft >= 0.5).astype(int))\n",
    "    precision = precision_score(y_test, (y_pred_proba_ft >= 0.5).astype(int))\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba_ft)\n",
    "    brier_score = brier_score_loss(y_test, y_pred_proba_ft)\n",
    "\n",
    "\n",
    "    results_Metric = pd.DataFrame({\n",
    "        'Metric': ['Accuracy', 'Recall', 'Precision', 'ROC AUC', 'Brier Score Loss'],\n",
    "        'Value': [accuracy, recall, precision, roc_auc, brier_score],\n",
    "        'SiteName': [f'{site_name}'] * 5,\n",
    "        'FineTune': ['Yes'] * 5,\n",
    "    })\n",
    "    results_Metric.to_csv(f'{output_directory}/result_metrics_{site_name}_ft.csv',index=False)\n",
    "\n",
    "\n",
    "    model_filename = f\"{output_directory}/lgbm_model_{site_name}_ft.txt\"\n",
    "\n",
    "    # Save the model using LightGBM's built-in function\n",
    "    model.save_model(model_filename)\n",
    "\n",
    "    print(results_Metric)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mortality_model)",
   "language": "python",
   "name": ".mortality_model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
