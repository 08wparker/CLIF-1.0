{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#!{sys.executable} -m pip3 install pandas numpy scikit-learn lightgbm matplotlib duckdb pyarrow\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import warnings\n",
    "import duckdb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (confusion_matrix, auc, roc_curve, accuracy_score, \n",
    "                             precision_score, recall_score, f1_score, \n",
    "                             precision_recall_curve, roc_auc_score, brier_score_loss)\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "# install pyarrow to work with parquet files\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "#pd.set_option('display.max_rows', None)\n",
    "random.seed(37)\n",
    "np.random.seed(37)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "con = duckdb.connect(database=\":memory:\")\n",
    "\n",
    "print(\"Setup complete\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control panel- User Input required\n",
    "\n",
    "Update root location, input filetype, site_name and confirm that race/ethnicity mapping correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter the location for your CLIF-1.0 directory\n",
    "root_location = 'C:/Users/vchaudha/OneDrive - rush.edu/CLIF-1.0-main'\n",
    "# either parquet or csv only\n",
    "filetype = 'csv'\n",
    "site_name='RUSH'\n",
    "\n",
    "race_map = {\n",
    "    'White': 'White',\n",
    "    'Black or African American': 'Black',\n",
    "    'Asian': 'Asian',\n",
    "    'Other': 'Others',\n",
    "    'Unknown': 'Others',\n",
    "    'Did Not Encounter': 'Others',\n",
    "    'Refusal': 'Others',\n",
    "    'American Indian or Alaska Native': 'Others',\n",
    "    'Native Hawaiian or Other Pacific Islander': 'Others',\n",
    "    np.nan: 'Others'\n",
    "}\n",
    "\n",
    "ethnicity_map = {\n",
    "    'Not Hispanic or Latino': 'Not Hispanic or Latino',\n",
    "    'Hispanic or Latino': 'Hispanic or Latino',\n",
    "    'Did Not Encounter': 'Others',\n",
    "    'Refusal': 'Others',\n",
    "    '*Unspecified': 'Others',\n",
    "    np.nan: 'Others'\n",
    "}\n",
    "\n",
    "finetune=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adt_filepath = f\"{root_location}/rclif/clif_adt.{filetype}\"\n",
    "encounter_filepath = f\"{root_location}/rclif/clif_encounter_demographics_dispo_clean.{filetype}\"\n",
    "limited_filepath = f\"{root_location}/rclif/clif_limited_identifiers.{filetype}\"\n",
    "demog_filepath = f\"{root_location}/rclif/clif_patient_demographics.{filetype}\"\n",
    "vitals_filepath = f\"{root_location}/rclif/clif_vitals_clean.{filetype}\"\n",
    "labs_filepath = f\"{root_location}/rclif/clif_labs_clean.{filetype}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filepath, filetype):\n",
    "    \"\"\"\n",
    "    Read data from file based on file type.\n",
    "    Parameters:\n",
    "        filepath (str): Path to the file.\n",
    "        filetype (str): Type of the file ('csv' or 'parquet').\n",
    "    Returns:\n",
    "        DataFrame: DataFrame containing the data.\n",
    "    \"\"\"\n",
    "    if filetype == 'csv':\n",
    "        return pd.read_csv(filepath)\n",
    "    elif filetype == 'parquet':\n",
    "        table = pq.read_table(filepath)\n",
    "        return table.to_pandas()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Please provide either 'csv' or 'parquet'.\")\n",
    "    \n",
    "\n",
    "def standardize_datetime(df):\n",
    "    \"\"\"\n",
    "    Ensure that all *_dttm variables are in the correct format.\n",
    "    Convert all datetime columns to a specific precision and remove timezone\n",
    "    Parameters:\n",
    "        DataFrame: DataFrame containing the data.\n",
    "    Returns:\n",
    "        DataFrame: DataFrame containing the data.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            # Here converting to 'datetime64[ns]' for uniformity and removing timezone with 'tz_convert(None)'\n",
    "            df[col] = df[col].dt.tz_convert(None) if df[col].dt.tz is not None else df[col]\n",
    "            # If you need to standardize to UTC and keep the timezone:\n",
    "            # df[col] = df[col].dt.tz_localize('UTC') if df[col].dt.tz is None else df[col].dt.tz_convert('UTC')\n",
    "    return df\n",
    "\n",
    "def get_sql_import(filetype):\n",
    "    if filetype == 'parquet':\n",
    "        return 'read_parquet'\n",
    "    if filetype == 'csv':\n",
    "        return 'read_csv_auto'\n",
    "\n",
    "sql_import = get_sql_import(filetype=filetype)\n",
    "\n",
    "# create output directory\n",
    "output_directory = os.path.join(os.getcwd(), 'output')\n",
    "os.makedirs(output_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = read_data(adt_filepath, filetype)\n",
    "encounter = read_data(encounter_filepath, filetype)\n",
    "limited = read_data(limited_filepath, filetype)\n",
    "demog = read_data(demog_filepath, filetype)\n",
    "\n",
    "# Apply the standardization function to each DataFrame\n",
    "location = standardize_datetime(location)\n",
    "encounter = standardize_datetime(encounter)\n",
    "limited = standardize_datetime(limited)\n",
    "demog = standardize_datetime(demog)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICU close to Admission\n",
    "\n",
    "1. Check ICU location_category between admission_dttmtime and 48hr stop from admission\n",
    "2. Check ICU stay at least 24 hr (for ICU - OR - ICU including OR in ICU stay 24hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "join=pd.merge(location[['encounter_id','location_category','in_dttm','out_dttm']],\\\n",
    "              limited[['encounter_id','admission_dttm']], on=['encounter_id'], how='left')\n",
    "\n",
    "icu_data=pd.merge(join,\\\n",
    "                  encounter[['encounter_id','age_at_admission','disposition']], on=['encounter_id'], how='left')\n",
    "\n",
    "\n",
    "icu_data['in_dttm'] = pd.to_datetime(icu_data['in_dttm'])\n",
    "icu_data['admission_dttm'] = pd.to_datetime(icu_data['admission_dttm'])\n",
    "icu_data['out_dttm'] = pd.to_datetime(icu_data['out_dttm'])\n",
    "#icu_data['age_at_admission'] = icu_data['age_at_admission'].astype(int)\n",
    "\n",
    "icu_48hr_check = icu_data[\n",
    "    (icu_data['location_category'] == 'ICU') &\n",
    "    (icu_data['in_dttm'] >= icu_data['admission_dttm']) &\n",
    "    (icu_data['in_dttm'] <= icu_data['admission_dttm'] + pd.Timedelta(hours=48)) &\n",
    "    (icu_data['in_dttm'].dt.year >= 2020) & (icu_data['in_dttm'].dt.year <= 2021) & \n",
    "    (icu_data['age_at_admission'] >= 18) & (icu_data['age_at_admission'].notna())\n",
    "]['encounter_id'].unique()\n",
    "\n",
    "icu_data=icu_data[icu_data['encounter_id'].isin(icu_48hr_check) & (icu_data['in_dttm'] <= icu_data['admission_dttm'] + pd.Timedelta(hours=72))].reset_index(drop=True)\n",
    "\n",
    "icu_data = icu_data.sort_values(by=['in_dttm']).reset_index(drop=True)\n",
    "\n",
    "icu_data[\"RANK\"]=icu_data.sort_values(by=['in_dttm'], ascending=True).groupby(\"encounter_id\")[\"in_dttm\"].rank(method=\"first\", ascending=True).astype(int)\n",
    "\n",
    "\n",
    "min_icu=icu_data[icu_data['location_category'] == 'ICU'].groupby('encounter_id')['RANK'].min()\n",
    "icu_data=pd.merge(icu_data, pd.DataFrame(zip(min_icu.index, min_icu.values), columns=['encounter_id', 'min_icu']), on='encounter_id', how='left')\n",
    "icu_data=icu_data[icu_data['RANK']>=icu_data['min_icu']].reset_index(drop=True)\n",
    "\n",
    "icu_data.loc[icu_data['location_category'] == 'OR', 'location_category'] = 'ICU'\n",
    "\n",
    "icu_data['group_id'] = (icu_data.groupby('encounter_id')['location_category'].shift() != icu_data['location_category']).astype(int)\n",
    "icu_data['group_id'] = icu_data.sort_values(by=['in_dttm'], ascending=True).groupby('encounter_id')['group_id'].cumsum()\n",
    "\n",
    "\n",
    "icu_data = icu_data.sort_values(by=['in_dttm'], ascending=True).groupby(['encounter_id', 'location_category', 'group_id']).agg(\n",
    "    min_in_dttm=('in_dttm', 'min'),\n",
    "    max_out_dttm=('out_dttm', 'max'),\n",
    "    admission_dttm=('admission_dttm', 'first'),\n",
    "    age=('age_at_admission', 'first'),\n",
    "    dispo=('disposition', 'first')\n",
    ").reset_index()\n",
    "\n",
    "min_icu=icu_data[icu_data['location_category'] == 'ICU'].groupby('encounter_id')['group_id'].min()\n",
    "icu_data=pd.merge(icu_data, pd.DataFrame(zip(min_icu.index, min_icu.values), columns=['encounter_id', 'min_icu']), on='encounter_id', how='left')\n",
    "\n",
    "icu_data=icu_data[(icu_data['min_icu']==icu_data['group_id']) &\n",
    "         (icu_data['max_out_dttm']-icu_data['min_in_dttm'] >= pd.Timedelta(hours=24))\n",
    "         ].reset_index(drop=True)\n",
    "\n",
    "\n",
    "icu_data['after_24hr']=icu_data['min_in_dttm'] + pd.Timedelta(hours=24)\n",
    "\n",
    "icu_data=icu_data[['encounter_id','min_in_dttm','after_24hr','age','dispo']]\n",
    "\n",
    "icu_data=pd.merge(icu_data,\\\n",
    "                  demog, on=['encounter_id'], how='left')[['encounter_id','min_in_dttm','after_24hr','age','dispo','sex','ethnicity','race']]\n",
    "icu_data=icu_data[~icu_data['sex'].isna()].reset_index(drop=True)\n",
    "icu_data['isfemale']=(icu_data['sex'].str.lower() == 'female').astype(int)\n",
    "icu_data['isdeathdispo'] = (icu_data['dispo'].str.contains('dead|expired', case=False, regex=True)).astype(int)\n",
    "\n",
    "icu_data['ethnicity'] = icu_data['ethnicity'].map(ethnicity_map)\n",
    "icu_data['race'] = icu_data['race'].map(race_map)\n",
    "\n",
    "\n",
    "del location,encounter,limited,demog"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd23a23b9104f1c8de37096cd54735f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vitals = con.execute(f'''\n",
    "    SELECT \n",
    "        encounter_id,\n",
    "        CAST(recorded_dttm AS datetime) AS recorded_dttm,\n",
    "        CAST(vital_value AS float) AS vital_value,\n",
    "        vital_category \n",
    "    FROM \n",
    "        {sql_import}('{vitals_filepath}')\n",
    "    WHERE \n",
    "        vital_category IN ('weight_kg', 'pulse', 'sbp', 'dbp', 'temp_c','height_inches') \n",
    "        AND encounter_id IN (SELECT DISTINCT encounter_id FROM icu_data);\n",
    "''').df()\n",
    "\n",
    "vitals=con.execute('''\n",
    "PIVOT vitals\n",
    "ON vital_category\n",
    "USING first(vital_value)\n",
    "GROUP BY encounter_id,recorded_dttm;\n",
    "''').df()\n",
    "\n",
    "vitals['height_meters'] = vitals['height_inches'] * 0.0254\n",
    "\n",
    "# Calculate BMI\n",
    "vitals['bmi'] = vitals['weight_kg'] / (vitals['height_meters'] ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "icu_data_agg=pd.merge(icu_data,vitals, on=['encounter_id'], how='left')\n",
    "icu_data_agg=icu_data_agg[(icu_data_agg['recorded_dttm'] >= icu_data_agg['min_in_dttm']) & (icu_data_agg['recorded_dttm'] <= icu_data_agg['after_24hr'])].reset_index(drop=True)\n",
    "\n",
    "icu_data_agg = icu_data_agg.groupby(['encounter_id']).agg(\n",
    "    min_bmi=('bmi', 'min'),\n",
    "    max_bmi=('bmi', 'max'),\n",
    "    avg_bmi=('bmi', 'mean'),\n",
    "    min_weight_kg=('weight_kg', 'min'),\n",
    "    max_weight_kg=('weight_kg', 'max'),\n",
    "    avg_weight_kg=('weight_kg', 'mean'),\n",
    "    min_pulse=('pulse', 'min'),\n",
    "    max_pulse=('pulse', 'max'),\n",
    "    avg_pulse=('pulse', 'mean'),\n",
    "    min_sbp=('sbp', 'min'),\n",
    "    max_sbp=('sbp', 'max'),\n",
    "    avg_sbp=('sbp', 'mean'),\n",
    "    min_dbp=('dbp', 'min'),\n",
    "    max_dbp=('dbp', 'max'),\n",
    "    avg_dbp=('dbp', 'mean'),\n",
    "    min_temp_c=('temp_c', 'min'),\n",
    "    max_temp_c=('temp_c', 'max'),\n",
    "    avg_temp_c=('temp_c', 'mean'),\n",
    ").reset_index()\n",
    "\n",
    "icu_data=pd.merge(icu_data,icu_data_agg, on=['encounter_id'], how='left')\n",
    "\n",
    "del vitals,icu_data_agg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bada2af0703b4c75b226a4142d102a39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labs = con.execute(f'''\n",
    "    SELECT \n",
    "        encounter_id,\n",
    "        CAST(lab_order_dttm AS datetime) AS lab_order_dttm,\n",
    "        TRY_CAST(lab_value AS float) AS lab_value,\n",
    "        lab_category\n",
    "    FROM \n",
    "         {sql_import}('{labs_filepath}')\n",
    "    WHERE \n",
    "        ((lab_category='monocyte'               and lab_type_name='standard') OR\n",
    "        (lab_category='lymphocyte'              and lab_type_name='standard') OR\n",
    "        (lab_category='basophil'                and lab_type_name='standard') OR\n",
    "        (lab_category='neutrophil'              and lab_type_name='standard') OR\n",
    "        (lab_category='albumin'                 and lab_type_name='standard') OR\n",
    "        (lab_category='ast'                     and lab_type_name='standard') OR\n",
    "        (lab_category='total_protein'           and lab_type_name='standard') OR\n",
    "        (lab_category='alkaline_phosphatase'    and lab_type_name='standard') OR\n",
    "        (lab_category='bilirubin_total'         and lab_type_name='standard') OR\n",
    "        (lab_category='bilirubin_conjugated'    and lab_type_name='standard') OR\n",
    "        (lab_category='calcium'                 and lab_type_name='standard') OR\n",
    "        (lab_category='chloride'                and lab_type_name='standard') OR\n",
    "        (lab_category='potassium'               and lab_type_name='standard') OR\n",
    "        (lab_category='sodium'                  and lab_type_name='standard') OR\n",
    "        (lab_category='glucose_serum'           and lab_type_name='standard') OR\n",
    "        (lab_category='hemoglobin'              and lab_type_name='standard') OR\n",
    "        (lab_category='platelet count'          and lab_type_name='standard') OR\n",
    "        (lab_category='wbc'                     and lab_type_name='standard'))\n",
    "        AND encounter_id IN (SELECT DISTINCT encounter_id FROM icu_data);\n",
    "''').df()\n",
    "\n",
    "labs=con.execute('''\n",
    "PIVOT labs\n",
    "ON lab_category\n",
    "USING first(lab_value)\n",
    "GROUP BY encounter_id,lab_order_dttm;\n",
    "''').df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "icu_data_agg=pd.merge(icu_data,labs, on=['encounter_id'], how='left')\n",
    "icu_data_agg=icu_data_agg[(icu_data_agg['lab_order_dttm'] >= icu_data_agg['min_in_dttm']) & (icu_data_agg['lab_order_dttm'] <= icu_data_agg['after_24hr'])].reset_index(drop=True)\n",
    "\n",
    "\n",
    "Lab_variables = [\n",
    "   'albumin', 'alkaline_phosphatase',\n",
    "       'ast', 'basophil', 'bilirubin_conjugated', 'bilirubin_total', 'calcium',\n",
    "       'chloride', 'hemoglobin', 'lymphocyte', 'monocyte', 'glucose_serum', \n",
    "       'neutrophil', 'potassium', 'sodium', 'total_protein','platelet count', \n",
    "       'wbc'\n",
    "]\n",
    "agg_dict = {var: ['min', 'max', 'mean'] for var in Lab_variables}\n",
    "\n",
    "icu_data_agg = icu_data_agg.groupby('encounter_id').agg(agg_dict).reset_index()\n",
    "\n",
    "icu_data_agg.columns = ['_'.join(col).strip() if col[1] else col[0] for col in icu_data_agg.columns.values]\n",
    "\n",
    "icu_data=pd.merge(icu_data,icu_data_agg, on=['encounter_id'], how='left')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_col=['isfemale','age', 'min_bmi', 'max_bmi', 'avg_bmi',\n",
    "       'min_weight_kg', 'max_weight_kg', 'avg_weight_kg', 'min_pulse',\n",
    "       'max_pulse', 'avg_pulse', 'min_sbp', 'max_sbp', 'avg_sbp', 'min_dbp',\n",
    "       'max_dbp', 'avg_dbp', 'min_temp_c', 'max_temp_c', 'avg_temp_c',\n",
    "       'albumin_min', 'albumin_max', 'albumin_mean',\n",
    "       'alkaline_phosphatase_min', 'alkaline_phosphatase_max',\n",
    "       'alkaline_phosphatase_mean', 'ast_min', 'ast_max', 'ast_mean',\n",
    "       'basophil_min', 'basophil_max', 'basophil_mean',\n",
    "       'bilirubin_conjugated_min', 'bilirubin_conjugated_max',\n",
    "       'bilirubin_conjugated_mean', 'bilirubin_total_min',\n",
    "       'bilirubin_total_max', 'bilirubin_total_mean', 'calcium_min',\n",
    "       'calcium_max', 'calcium_mean', 'chloride_min', 'chloride_max',\n",
    "       'chloride_mean', 'glucose_serum_min', 'glucose_serum_max',\n",
    "       'glucose_serum_mean', 'hemoglobin_min', 'hemoglobin_max',\n",
    "       'hemoglobin_mean', 'lymphocyte_min', 'lymphocyte_max',\n",
    "       'lymphocyte_mean', 'monocyte_min', 'monocyte_max', 'monocyte_mean',\n",
    "       'neutrophil_min', 'neutrophil_max', 'neutrophil_mean',\n",
    "       'platelet count_min', 'platelet count_max', 'platelet count_mean',\n",
    "       'potassium_min', 'potassium_max', 'potassium_mean', 'sodium_min',\n",
    "       'sodium_max', 'sodium_mean', 'total_protein_min', 'total_protein_max',\n",
    "       'total_protein_mean', 'wbc_min', 'wbc_max', 'wbc_mean']\n",
    "\n",
    "model=lgb.Booster(model_file=f'{root_location}/projects/Mortality_model/models/lgbm_model_20240529-110556.txt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### basic metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "      <th>SiteName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.954284</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.756164</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.667473</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ROC AUC</td>\n",
       "      <td>0.950114</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brier Score Loss</td>\n",
       "      <td>0.033514</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Metric     Value SiteName\n",
       "0          Accuracy  0.954284     RUSH\n",
       "1            Recall  0.756164     RUSH\n",
       "2         Precision  0.667473     RUSH\n",
       "3           ROC AUC  0.950114     RUSH\n",
       "4  Brier Score Loss  0.033514     RUSH"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test=icu_data[model_col]\n",
    "y_test=icu_data['isdeathdispo']\n",
    "\n",
    "y_pred_proba = model.predict(X_test)\n",
    "icu_data['pred_proba'] = y_pred_proba\n",
    "\n",
    "thr=0.194\n",
    "accuracy = accuracy_score(y_test, (y_pred_proba >= thr).astype(int))\n",
    "recall = recall_score(y_test, (y_pred_proba >= thr).astype(int))\n",
    "precision = precision_score(y_test, (y_pred_proba >= thr).astype(int))\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "brier_score = brier_score_loss(y_test, y_pred_proba)\n",
    "\n",
    "\n",
    "results_Metric = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Recall', 'Precision', 'ROC AUC', 'Brier Score Loss'],\n",
    "    'Value': [accuracy, recall, precision, roc_auc, brier_score],\n",
    "    'SiteName': [f'{site_name}'] * 5\n",
    "})\n",
    "\n",
    "results_Metric.to_csv(f'{output_directory}/result_metrics_{site_name}.csv',index=False)\n",
    "results_Metric"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### probablity table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_label</th>\n",
       "      <th>site_proba</th>\n",
       "      <th>Site_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.021630</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.015260</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.017889</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.005896</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.013865</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   site_label   site_proba Site_name\n",
       "0            0    0.021630      RUSH\n",
       "1            0    0.015260      RUSH\n",
       "2            0    0.017889      RUSH\n",
       "3            0    0.005896      RUSH\n",
       "4            0    0.013865      RUSH"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_df_lgbm = pd.DataFrame({'site_label ':y_test, 'site_proba': y_pred_proba,'Site_name':f\"{site_name}\" })\n",
    "prob_df_lgbm.to_csv(f'{output_directory}/Model_probabilities_{site_name}.csv',index=False)\n",
    "prob_df_lgbm.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model fairness test accross 'race', 'ethnicity', 'sex'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(data, true_col, pred_prob_col, subgroup_cols):\n",
    "    results = []\n",
    "    total_count = len(data)\n",
    "\n",
    "    for subgroup_col in subgroup_cols:\n",
    "       \n",
    "        filtered_data = data.dropna(subset=[subgroup_col])\n",
    "        \n",
    "        for group in filtered_data[subgroup_col].unique():\n",
    "            subgroup_data = filtered_data[filtered_data[subgroup_col] == group]\n",
    "            group_count = len(subgroup_data)\n",
    "            proportion = group_count / total_count\n",
    "\n",
    "            if np.unique(subgroup_data[true_col]).size > 1:  # Check if both classes are present\n",
    "                auc = roc_auc_score(subgroup_data[true_col], subgroup_data[pred_prob_col])\n",
    "                tn, fp, fn, tp = confusion_matrix(subgroup_data[true_col], (subgroup_data[pred_prob_col] > thr).astype(int)).ravel()\n",
    "                ppv = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "                result = {'Subgroup': subgroup_col, 'Group': group, 'AUC': auc, 'PPV': ppv, 'Group Count': group_count, 'Total Count': total_count, 'Proportion': proportion, 'site_name':f'{site_name}'}\n",
    "            else:\n",
    "                result = {'Subgroup': subgroup_col, 'Group': group, 'AUC': 'Not defined', 'PPV': 'Not applicable', 'Group Count': group_count, 'Total Count': total_count, 'Proportion': proportion, 'site_name':f'{site_name}'}\n",
    "            \n",
    "            results.append(result)\n",
    "    \n",
    "   \n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "result_df = calculate_metrics(icu_data, 'isdeathdispo', 'pred_proba', ['race', 'ethnicity', 'sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subgroup</th>\n",
       "      <th>Group</th>\n",
       "      <th>AUC</th>\n",
       "      <th>PPV</th>\n",
       "      <th>Group Count</th>\n",
       "      <th>Total Count</th>\n",
       "      <th>Proportion</th>\n",
       "      <th>site_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>race</td>\n",
       "      <td>White</td>\n",
       "      <td>0.945829</td>\n",
       "      <td>0.654412</td>\n",
       "      <td>3718</td>\n",
       "      <td>9909</td>\n",
       "      <td>0.375214</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>race</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.951510</td>\n",
       "      <td>0.665072</td>\n",
       "      <td>1846</td>\n",
       "      <td>9909</td>\n",
       "      <td>0.186295</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>race</td>\n",
       "      <td>Black</td>\n",
       "      <td>0.950892</td>\n",
       "      <td>0.682692</td>\n",
       "      <td>4016</td>\n",
       "      <td>9909</td>\n",
       "      <td>0.405288</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>race</td>\n",
       "      <td>Asian</td>\n",
       "      <td>0.962554</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>329</td>\n",
       "      <td>9909</td>\n",
       "      <td>0.033202</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ethnicity</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>0.952080</td>\n",
       "      <td>0.670492</td>\n",
       "      <td>7872</td>\n",
       "      <td>9909</td>\n",
       "      <td>0.794429</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ethnicity</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>0.945920</td>\n",
       "      <td>0.651961</td>\n",
       "      <td>1981</td>\n",
       "      <td>9909</td>\n",
       "      <td>0.199919</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ethnicity</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.862245</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>56</td>\n",
       "      <td>9909</td>\n",
       "      <td>0.005651</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sex</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.951614</td>\n",
       "      <td>0.651452</td>\n",
       "      <td>5220</td>\n",
       "      <td>9909</td>\n",
       "      <td>0.526794</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sex</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.947765</td>\n",
       "      <td>0.689855</td>\n",
       "      <td>4689</td>\n",
       "      <td>9909</td>\n",
       "      <td>0.473206</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Subgroup                   Group       AUC       PPV  Group Count  \\\n",
       "0       race                   White  0.945829  0.654412         3718   \n",
       "1       race                  Others  0.951510  0.665072         1846   \n",
       "2       race                   Black  0.950892  0.682692         4016   \n",
       "3       race                   Asian  0.962554  0.647059          329   \n",
       "4  ethnicity  Not Hispanic or Latino  0.952080  0.670492         7872   \n",
       "5  ethnicity      Hispanic or Latino  0.945920  0.651961         1981   \n",
       "6  ethnicity                  Others  0.862245  0.769231           56   \n",
       "7        sex                    Male  0.951614  0.651452         5220   \n",
       "8        sex                  Female  0.947765  0.689855         4689   \n",
       "\n",
       "   Total Count  Proportion site_name  \n",
       "0         9909    0.375214      RUSH  \n",
       "1         9909    0.186295      RUSH  \n",
       "2         9909    0.405288      RUSH  \n",
       "3         9909    0.033202      RUSH  \n",
       "4         9909    0.794429      RUSH  \n",
       "5         9909    0.199919      RUSH  \n",
       "6         9909    0.005651      RUSH  \n",
       "7         9909    0.526794      RUSH  \n",
       "8         9909    0.473206      RUSH  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.to_csv(f'{output_directory}/fairness_test_{site_name}.csv',index=False)\n",
    "result_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### thrshold check at site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_percentile(target_var, pred_proba):\n",
    "    #thr_list = [0.99,0.97, 0.95,0.90,0.80,0.70,0.60,0.50,0.40,0.30,0.20,0.10]\n",
    "    thr_list = np.arange(1, 0, -0.01)\n",
    "    col = ['N Percentile', 'Thr Value','TN','FP','FN','TP','Sensitivity','Specificity','PPV', 'NPV' ,'Recall','Accuracy','site_name']\n",
    "    result = pd.DataFrame(columns = col)\n",
    "    i = 0\n",
    "    \n",
    "    for thr in thr_list: \n",
    "        prob = pd.DataFrame()\n",
    "        prob['target_var'] = target_var\n",
    "        prob['pred_proba'] = pred_proba\n",
    "\n",
    "        thr_value = prob['pred_proba'].quantile(thr)\n",
    "        prob['pred_proba_bin'] = np.where(prob['pred_proba'] >= thr_value, 1, 0)\n",
    "        tn,fp,fn,tp = confusion_matrix(prob['target_var'], prob['pred_proba_bin']).ravel()\n",
    "\n",
    "        sensitivity = tp/(tp+fn)\n",
    "        specificity = tn/(tn+fp)\n",
    "        ppv = tp/(tp+fp)\n",
    "        npv = tn/(tn+fn)\n",
    "        recall = tp/(tp+fn)\n",
    "        acc = (tp+tn)/(tp+fn+tn+fp)\n",
    "        n_prec = 'Top '+ str(np.round((1 - thr) * 100,0))+ \"%\"\n",
    "        result.loc[i] = [n_prec,thr_value,tn,fp,fn,tp,sensitivity,specificity ,ppv,npv, recall, acc,f'{site_name}']\n",
    "        i+=1\n",
    "    return result\n",
    "topn=top_n_percentile(y_test,y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N Percentile</th>\n",
       "      <th>Thr Value</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TP</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>PPV</th>\n",
       "      <th>NPV</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>site_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top 0.0%</td>\n",
       "      <td>0.990239</td>\n",
       "      <td>9179</td>\n",
       "      <td>0</td>\n",
       "      <td>729</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.926423</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>0.926431</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Top 1.0%</td>\n",
       "      <td>0.785633</td>\n",
       "      <td>9174</td>\n",
       "      <td>5</td>\n",
       "      <td>635</td>\n",
       "      <td>95</td>\n",
       "      <td>0.130137</td>\n",
       "      <td>0.999455</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.935264</td>\n",
       "      <td>0.130137</td>\n",
       "      <td>0.935412</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Top 2.0%</td>\n",
       "      <td>0.654390</td>\n",
       "      <td>9172</td>\n",
       "      <td>7</td>\n",
       "      <td>538</td>\n",
       "      <td>192</td>\n",
       "      <td>0.263014</td>\n",
       "      <td>0.999237</td>\n",
       "      <td>0.964824</td>\n",
       "      <td>0.944593</td>\n",
       "      <td>0.263014</td>\n",
       "      <td>0.944999</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Top 3.0%</td>\n",
       "      <td>0.542831</td>\n",
       "      <td>9163</td>\n",
       "      <td>16</td>\n",
       "      <td>448</td>\n",
       "      <td>282</td>\n",
       "      <td>0.386301</td>\n",
       "      <td>0.998257</td>\n",
       "      <td>0.946309</td>\n",
       "      <td>0.953387</td>\n",
       "      <td>0.386301</td>\n",
       "      <td>0.953174</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Top 4.0%</td>\n",
       "      <td>0.457467</td>\n",
       "      <td>9147</td>\n",
       "      <td>32</td>\n",
       "      <td>365</td>\n",
       "      <td>365</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.996514</td>\n",
       "      <td>0.919395</td>\n",
       "      <td>0.961627</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.959935</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Top 5.0%</td>\n",
       "      <td>0.360236</td>\n",
       "      <td>9112</td>\n",
       "      <td>67</td>\n",
       "      <td>301</td>\n",
       "      <td>429</td>\n",
       "      <td>0.587671</td>\n",
       "      <td>0.992701</td>\n",
       "      <td>0.864919</td>\n",
       "      <td>0.968023</td>\n",
       "      <td>0.587671</td>\n",
       "      <td>0.962862</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  N Percentile  Thr Value    TN  FP   FN   TP  Sensitivity  Specificity  \\\n",
       "0     Top 0.0%   0.990239  9179   0  729    1     0.001370     1.000000   \n",
       "1     Top 1.0%   0.785633  9174   5  635   95     0.130137     0.999455   \n",
       "2     Top 2.0%   0.654390  9172   7  538  192     0.263014     0.999237   \n",
       "3     Top 3.0%   0.542831  9163  16  448  282     0.386301     0.998257   \n",
       "4     Top 4.0%   0.457467  9147  32  365  365     0.500000     0.996514   \n",
       "5     Top 5.0%   0.360236  9112  67  301  429     0.587671     0.992701   \n",
       "\n",
       "        PPV       NPV    Recall  Accuracy site_name  \n",
       "0  1.000000  0.926423  0.001370  0.926431      RUSH  \n",
       "1  0.950000  0.935264  0.130137  0.935412      RUSH  \n",
       "2  0.964824  0.944593  0.263014  0.944999      RUSH  \n",
       "3  0.946309  0.953387  0.386301  0.953174      RUSH  \n",
       "4  0.919395  0.961627  0.500000  0.959935      RUSH  \n",
       "5  0.864919  0.968023  0.587671  0.962862      RUSH  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topn.to_csv(f'{output_directory}/Top_N_percentile_PPV_{site_name}.csv',index=False)\n",
    "topn.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RUSH THR Top N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Thr Value</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TP</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>PPV</th>\n",
       "      <th>NPV</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>site_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.194</td>\n",
       "      <td>8904</td>\n",
       "      <td>275</td>\n",
       "      <td>178</td>\n",
       "      <td>552</td>\n",
       "      <td>0.756164</td>\n",
       "      <td>0.97004</td>\n",
       "      <td>0.667473</td>\n",
       "      <td>0.980401</td>\n",
       "      <td>0.756164</td>\n",
       "      <td>0.954284</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Thr Value    TN   FP   FN   TP  Sensitivity  Specificity       PPV  \\\n",
       "0      0.194  8904  275  178  552     0.756164      0.97004  0.667473   \n",
       "\n",
       "        NPV    Recall  Accuracy site_name  \n",
       "0  0.980401  0.756164  0.954284      RUSH  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = ['Thr Value','TN','FP','FN','TP','Sensitivity','Specificity','PPV', 'NPV' ,'Recall','Accuracy','site_name']\n",
    "result = pd.DataFrame(columns = col)\n",
    "\n",
    "prob = pd.DataFrame()\n",
    "prob['target_var'] = y_test\n",
    "prob['pred_proba'] = y_pred_proba\n",
    "\n",
    "prob['pred_proba_bin'] = np.where(prob['pred_proba'] >= thr, 1, 0)\n",
    "tn,fp,fn,tp = confusion_matrix(prob['target_var'], prob['pred_proba_bin']).ravel()\n",
    "\n",
    "sensitivity = tp/(tp+fn)\n",
    "specificity = tn/(tn+fp)\n",
    "ppv = tp/(tp+fp)\n",
    "npv = tn/(tn+fn)\n",
    "recall = tp/(tp+fn)\n",
    "acc = (tp+tn)/(tp+fn+tn+fp)\n",
    "n_prec = 'Top '+ str((1 - thr))+ \"%\"\n",
    "result.loc[0] = [thr,tn,fp,fn,tp,sensitivity,specificity ,ppv,npv, recall, acc,f'{site_name}']\n",
    "\n",
    "result.to_csv(f'{output_directory}/Top_N_percentile_atRushThr_{site_name}.csv',index=False)\n",
    "result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mortality_model)",
   "language": "python",
   "name": ".mortality_model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
