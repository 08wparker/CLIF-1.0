{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#!{sys.executable} -m pip3 install pandas numpy scikit-learn lightgbm matplotlib duckdb pyarrow\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import warnings\n",
    "import duckdb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (confusion_matrix, auc, roc_curve, accuracy_score, \n",
    "                             precision_score, recall_score, f1_score, \n",
    "                             precision_recall_curve, roc_auc_score, brier_score_loss)\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "# install pyarrow to work with parquet files\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "#pd.set_option('display.max_rows', None)\n",
    "random.seed(37)\n",
    "np.random.seed(37)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "con = duckdb.connect(database=\":memory:\")\n",
    "\n",
    "print(\"Setup complete\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control panel- User Input required\n",
    "\n",
    "Update root location, input filetype, site_name and confirm that race/ethnicity mapping correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter the location for your CLIF-1.0 directory\n",
    "root_location = 'C:/Users/vchaudha/OneDrive - rush.edu/CLIF-1.0-main'\n",
    "# either parquet or csv only\n",
    "filetype = 'csv'\n",
    "site_name='RUSH'\n",
    "\n",
    "race_map = {\n",
    "    'White': 'White',\n",
    "    'Black or African American': 'Black',\n",
    "    'Asian': 'Asian',\n",
    "    'Other': 'Others',\n",
    "    'Unknown': 'Others',\n",
    "    'Did Not Encounter': 'Others',\n",
    "    'Refusal': 'Others',\n",
    "    'American Indian or Alaska Native': 'Others',\n",
    "    'Native Hawaiian or Other Pacific Islander': 'Others',\n",
    "    np.nan: 'Others'\n",
    "}\n",
    "\n",
    "ethnicity_map = {\n",
    "    'Not Hispanic or Latino': 'Not Hispanic or Latino',\n",
    "    'Hispanic or Latino': 'Hispanic or Latino',\n",
    "    'Did Not Encounter': 'Others',\n",
    "    'Refusal': 'Others',\n",
    "    '*Unspecified': 'Others',\n",
    "    np.nan: 'Others'\n",
    "}\n",
    "\n",
    "finetune=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adt_filepath = f\"{root_location}/rclif/clif_adt.{filetype}\"\n",
    "encounter_filepath = f\"{root_location}/rclif/clif_encounter_demographics_dispo.{filetype}\"\n",
    "limited_filepath = f\"{root_location}/rclif/clif_limited_identifiers.{filetype}\"\n",
    "demog_filepath = f\"{root_location}/rclif/clif_patient_demographics.{filetype}\"\n",
    "vitals_filepath = f\"{root_location}/rclif/clif_vitals.{filetype}\"\n",
    "labs_filepath = f\"{root_location}/rclif/clif_labs.{filetype}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filepath, filetype):\n",
    "    \"\"\"\n",
    "    Read data from file based on file type.\n",
    "    Parameters:\n",
    "        filepath (str): Path to the file.\n",
    "        filetype (str): Type of the file ('csv' or 'parquet').\n",
    "    Returns:\n",
    "        DataFrame: DataFrame containing the data.\n",
    "    \"\"\"\n",
    "    if filetype == 'csv':\n",
    "        return pd.read_csv(filepath)\n",
    "    elif filetype == 'parquet':\n",
    "        table = pq.read_table(filepath)\n",
    "        return table.to_pandas()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Please provide either 'csv' or 'parquet'.\")\n",
    "    \n",
    "\n",
    "def standardize_datetime(df):\n",
    "    \"\"\"\n",
    "    Ensure that all *_dttm variables are in the correct format.\n",
    "    Convert all datetime columns to a specific precision and remove timezone\n",
    "    Parameters:\n",
    "        DataFrame: DataFrame containing the data.\n",
    "    Returns:\n",
    "        DataFrame: DataFrame containing the data.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            # Here converting to 'datetime64[ns]' for uniformity and removing timezone with 'tz_convert(None)'\n",
    "            df[col] = df[col].dt.tz_convert(None) if df[col].dt.tz is not None else df[col]\n",
    "            # If you need to standardize to UTC and keep the timezone:\n",
    "            # df[col] = df[col].dt.tz_localize('UTC') if df[col].dt.tz is None else df[col].dt.tz_convert('UTC')\n",
    "    return df\n",
    "\n",
    "def get_sql_import(filetype):\n",
    "    if filetype == 'parquet':\n",
    "        return 'read_parquet'\n",
    "    if filetype == 'csv':\n",
    "        return 'read_csv_auto'\n",
    "\n",
    "sql_import = get_sql_import(filetype=filetype)\n",
    "\n",
    "# create output directory\n",
    "output_directory = os.path.join(os.getcwd(), 'output')\n",
    "os.makedirs(output_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = read_data(adt_filepath, filetype)\n",
    "encounter = read_data(encounter_filepath, filetype)\n",
    "limited = read_data(limited_filepath, filetype)\n",
    "demog = read_data(demog_filepath, filetype)\n",
    "\n",
    "# Apply the standardization function to each DataFrame\n",
    "location = standardize_datetime(location)\n",
    "encounter = standardize_datetime(encounter)\n",
    "limited = standardize_datetime(limited)\n",
    "demog = standardize_datetime(demog)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICU close to Admission\n",
    "\n",
    "1. Check ICU location_category between admission_dttmtime and 48hr stop from admission\n",
    "2. Check ICU stay at least 24 hr (for ICU - OR - ICU including OR in ICU stay 24hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "join=pd.merge(location[['encounter_id','location_category','in_dttm','out_dttm']],\\\n",
    "              limited[['encounter_id','admission_dttm']], on=['encounter_id'], how='left')\n",
    "\n",
    "icu_data=pd.merge(join,\\\n",
    "                  encounter[['encounter_id','age_at_admission','disposition']], on=['encounter_id'], how='left')\n",
    "\n",
    "\n",
    "icu_data['in_dttm'] = pd.to_datetime(icu_data['in_dttm'])\n",
    "icu_data['admission_dttm'] = pd.to_datetime(icu_data['admission_dttm'])\n",
    "icu_data['out_dttm'] = pd.to_datetime(icu_data['out_dttm'])\n",
    "icu_data['age_at_admission'] = icu_data['age_at_admission'].astype(int)\n",
    "\n",
    "icu_48hr_check = icu_data[\n",
    "    (icu_data['location_category'] == 'ICU') &\n",
    "    (icu_data['in_dttm'] >= icu_data['admission_dttm']) &\n",
    "    (icu_data['in_dttm'] <= icu_data['admission_dttm'] + pd.Timedelta(hours=48)) &\n",
    "    (icu_data['in_dttm'].dt.year >= 2020) & (icu_data['in_dttm'].dt.year <= 2022) & \n",
    "    (icu_data['age_at_admission'] >= 18) & (icu_data['age_at_admission'].notna())\n",
    "]['encounter_id'].unique()\n",
    "\n",
    "icu_data=icu_data[icu_data['encounter_id'].isin(icu_48hr_check) & (icu_data['in_dttm'] <= icu_data['admission_dttm'] + pd.Timedelta(hours=72))].reset_index(drop=True)\n",
    "\n",
    "icu_data = icu_data.sort_values(by=['in_dttm']).reset_index(drop=True)\n",
    "\n",
    "icu_data[\"RANK\"]=icu_data.sort_values(by=['in_dttm'], ascending=True).groupby(\"encounter_id\")[\"in_dttm\"].rank(method=\"first\", ascending=True).astype(int)\n",
    "\n",
    "\n",
    "min_icu=icu_data[icu_data['location_category'] == 'ICU'].groupby('encounter_id')['RANK'].min()\n",
    "icu_data=pd.merge(icu_data, pd.DataFrame(zip(min_icu.index, min_icu.values), columns=['encounter_id', 'min_icu']), on='encounter_id', how='left')\n",
    "icu_data=icu_data[icu_data['RANK']>=icu_data['min_icu']].reset_index(drop=True)\n",
    "\n",
    "icu_data.loc[icu_data['location_category'] == 'OR', 'location_category'] = 'ICU'\n",
    "\n",
    "icu_data['group_id'] = (icu_data.groupby('encounter_id')['location_category'].shift() != icu_data['location_category']).astype(int)\n",
    "icu_data['group_id'] = icu_data.sort_values(by=['in_dttm'], ascending=True).groupby('encounter_id')['group_id'].cumsum()\n",
    "\n",
    "\n",
    "icu_data = icu_data.sort_values(by=['in_dttm'], ascending=True).groupby(['encounter_id', 'location_category', 'group_id']).agg(\n",
    "    min_in_dttm=('in_dttm', 'min'),\n",
    "    max_out_dttm=('out_dttm', 'max'),\n",
    "    admission_dttm=('admission_dttm', 'first'),\n",
    "    age=('age_at_admission', 'first'),\n",
    "    dispo=('disposition', 'first')\n",
    ").reset_index()\n",
    "\n",
    "min_icu=icu_data[icu_data['location_category'] == 'ICU'].groupby('encounter_id')['group_id'].min()\n",
    "icu_data=pd.merge(icu_data, pd.DataFrame(zip(min_icu.index, min_icu.values), columns=['encounter_id', 'min_icu']), on='encounter_id', how='left')\n",
    "\n",
    "icu_data=icu_data[(icu_data['min_icu']==icu_data['group_id']) &\n",
    "         (icu_data['max_out_dttm']-icu_data['min_in_dttm'] >= pd.Timedelta(hours=24))\n",
    "         ].reset_index(drop=True)\n",
    "\n",
    "\n",
    "icu_data['after_24hr']=icu_data['min_in_dttm'] + pd.Timedelta(hours=24)\n",
    "\n",
    "icu_data=icu_data[['encounter_id','min_in_dttm','after_24hr','age','dispo']]\n",
    "\n",
    "icu_data=pd.merge(icu_data,\\\n",
    "                  demog, on=['encounter_id'], how='left')[['encounter_id','min_in_dttm','after_24hr','age','dispo','sex','ethnicity','race']]\n",
    "icu_data=icu_data[~icu_data['sex'].isna()].reset_index(drop=True)\n",
    "icu_data['isfemale']=(icu_data['sex'].str.lower() == 'female').astype(int)\n",
    "icu_data['isdeathdispo'] = (icu_data['dispo'].str.contains('dead|expired', case=False, regex=True)).astype(int)\n",
    "\n",
    "icu_data['ethnicity'] = icu_data['ethnicity'].map(ethnicity_map)\n",
    "icu_data['race'] = icu_data['race'].map(race_map)\n",
    "\n",
    "\n",
    "del location,encounter,limited,demog"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14a1f23762444eba72bfbf34a0c2fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vitals = con.execute(f'''\n",
    "    SELECT \n",
    "        encounter_id,\n",
    "        CAST(recorded_dttm AS datetime) AS recorded_dttm,\n",
    "        CAST(vital_value AS float) AS vital_value,\n",
    "        vital_category \n",
    "    FROM \n",
    "        {sql_import}('{vitals_filepath}')\n",
    "    WHERE \n",
    "        vital_category IN ('weight_kg', 'pulse', 'sbp', 'dbp', 'temp_c','height_inches') \n",
    "        AND encounter_id IN (SELECT DISTINCT encounter_id FROM icu_data);\n",
    "''').df()\n",
    "\n",
    "vitals=con.execute('''\n",
    "PIVOT vitals\n",
    "ON vital_category\n",
    "USING first(vital_value)\n",
    "GROUP BY encounter_id,recorded_dttm;\n",
    "''').df()\n",
    "\n",
    "vitals['height_meters'] = vitals['height_inches'] * 0.0254\n",
    "\n",
    "# Calculate BMI\n",
    "vitals['bmi'] = vitals['weight_kg'] / (vitals['height_meters'] ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "icu_data_agg=pd.merge(icu_data,vitals, on=['encounter_id'], how='left')\n",
    "icu_data_agg=icu_data_agg[(icu_data_agg['recorded_dttm'] >= icu_data_agg['min_in_dttm']) & (icu_data_agg['recorded_dttm'] <= icu_data_agg['after_24hr'])].reset_index(drop=True)\n",
    "\n",
    "icu_data_agg = icu_data_agg.groupby(['encounter_id']).agg(\n",
    "    min_bmi=('bmi', 'min'),\n",
    "    max_bmi=('bmi', 'max'),\n",
    "    avg_bmi=('bmi', 'mean'),\n",
    "    min_weight_kg=('weight_kg', 'min'),\n",
    "    max_weight_kg=('weight_kg', 'max'),\n",
    "    avg_weight_kg=('weight_kg', 'mean'),\n",
    "    min_pulse=('pulse', 'min'),\n",
    "    max_pulse=('pulse', 'max'),\n",
    "    avg_pulse=('pulse', 'mean'),\n",
    "    min_sbp=('sbp', 'min'),\n",
    "    max_sbp=('sbp', 'max'),\n",
    "    avg_sbp=('sbp', 'mean'),\n",
    "    min_dbp=('dbp', 'min'),\n",
    "    max_dbp=('dbp', 'max'),\n",
    "    avg_dbp=('dbp', 'mean'),\n",
    "    min_temp_c=('temp_c', 'min'),\n",
    "    max_temp_c=('temp_c', 'max'),\n",
    "    avg_temp_c=('temp_c', 'mean'),\n",
    ").reset_index()\n",
    "\n",
    "icu_data=pd.merge(icu_data,icu_data_agg, on=['encounter_id'], how='left')\n",
    "\n",
    "del vitals,icu_data_agg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a172b3b5270242beb516f717b3e31761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labs = con.execute(f'''\n",
    "    SELECT \n",
    "        encounter_id,\n",
    "        CAST(lab_order_dttm AS datetime) AS lab_order_dttm,\n",
    "        TRY_CAST(lab_value AS float) AS lab_value,\n",
    "        lab_category\n",
    "    FROM \n",
    "         {sql_import}('{labs_filepath}')\n",
    "    WHERE \n",
    "         ((lab_category='monocyte'               and lab_type_name='standard') OR\n",
    "        (lab_category='lymphocyte'              and lab_type_name='standard') OR\n",
    "        (lab_category='basophil'                and lab_type_name='standard') OR\n",
    "        (lab_category='neutrophil'              and lab_type_name='standard') OR\n",
    "        (lab_category='albumin'                 and lab_type_name='standard') OR\n",
    "        (lab_category='ast'                     and lab_type_name='standard') OR\n",
    "        (lab_category='total_protein'           and lab_type_name='standard') OR\n",
    "        (lab_category='alkaline_phosphatase'    and lab_type_name='standard') OR\n",
    "        (lab_category='bilirubin_total'         and lab_type_name='standard') OR\n",
    "        (lab_category='bilirubin_conjugated'    and lab_type_name='standard') OR\n",
    "        (lab_category='calcium'                 and lab_type_name='standard') OR\n",
    "        (lab_category='chloride'                and lab_type_name='standard') OR\n",
    "        (lab_category='potassium'               and lab_type_name='standard') OR\n",
    "        (lab_category='sodium'                  and lab_type_name='standard') OR\n",
    "        (lab_category='glucose_serum'           and lab_type_name='standard') OR\n",
    "        (lab_category='hemoglobin'              and lab_type_name='standard') OR\n",
    "        (lab_category='platelet count'          and lab_type_name='standard') OR\n",
    "        (lab_category='wbc'                     and lab_type_name='standard'))\n",
    "        AND encounter_id IN (SELECT DISTINCT encounter_id FROM icu_data);\n",
    "''').df()\n",
    "\n",
    "labs=con.execute('''\n",
    "PIVOT labs\n",
    "ON lab_category\n",
    "USING first(lab_value)\n",
    "GROUP BY encounter_id,lab_order_dttm;\n",
    "''').df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "icu_data_agg=pd.merge(icu_data,labs, on=['encounter_id'], how='left')\n",
    "icu_data_agg=icu_data_agg[(icu_data_agg['lab_order_dttm'] >= icu_data_agg['min_in_dttm']) & (icu_data_agg['lab_order_dttm'] <= icu_data_agg['after_24hr'])].reset_index(drop=True)\n",
    "\n",
    "\n",
    "Lab_variables = [\n",
    "   'albumin', 'alkaline_phosphatase',\n",
    "       'ast', 'basophil', 'bilirubin_conjugated', 'bilirubin_total', 'calcium',\n",
    "       'chloride', 'hemoglobin', 'lymphocyte', 'monocyte', 'glucose_serum', \n",
    "       'neutrophil', 'potassium', 'sodium', 'total_protein','platelet count', \n",
    "       'wbc'\n",
    "]\n",
    "agg_dict = {var: ['min', 'max', 'mean'] for var in Lab_variables}\n",
    "\n",
    "icu_data_agg = icu_data_agg.groupby('encounter_id').agg(agg_dict).reset_index()\n",
    "\n",
    "icu_data_agg.columns = ['_'.join(col).strip() if col[1] else col[0] for col in icu_data_agg.columns.values]\n",
    "\n",
    "icu_data=pd.merge(icu_data,icu_data_agg, on=['encounter_id'], how='left')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_col=['isfemale','age', 'min_bmi', 'max_bmi', 'avg_bmi',\n",
    "       'min_weight_kg', 'max_weight_kg', 'avg_weight_kg', 'min_pulse',\n",
    "       'max_pulse', 'avg_pulse', 'min_sbp', 'max_sbp', 'avg_sbp', 'min_dbp',\n",
    "       'max_dbp', 'avg_dbp', 'min_temp_c', 'max_temp_c', 'avg_temp_c',\n",
    "       'albumin_min', 'albumin_max', 'albumin_mean',\n",
    "       'alkaline_phosphatase_min', 'alkaline_phosphatase_max',\n",
    "       'alkaline_phosphatase_mean', 'ast_min', 'ast_max', 'ast_mean',\n",
    "       'basophil_min', 'basophil_max', 'basophil_mean',\n",
    "       'bilirubin_conjugated_min', 'bilirubin_conjugated_max',\n",
    "       'bilirubin_conjugated_mean', 'bilirubin_total_min',\n",
    "       'bilirubin_total_max', 'bilirubin_total_mean', 'calcium_min',\n",
    "       'calcium_max', 'calcium_mean', 'chloride_min', 'chloride_max',\n",
    "       'chloride_mean', 'glucose_serum_min', 'glucose_serum_max',\n",
    "       'glucose_serum_mean', 'hemoglobin_min', 'hemoglobin_max',\n",
    "       'hemoglobin_mean', 'lymphocyte_min', 'lymphocyte_max',\n",
    "       'lymphocyte_mean', 'monocyte_min', 'monocyte_max', 'monocyte_mean',\n",
    "       'neutrophil_min', 'neutrophil_max', 'neutrophil_mean',\n",
    "       'platelet count_min', 'platelet count_max', 'platelet count_mean',\n",
    "       'potassium_min', 'potassium_max', 'potassium_mean', 'sodium_min',\n",
    "       'sodium_max', 'sodium_mean', 'total_protein_min', 'total_protein_max',\n",
    "       'total_protein_mean', 'wbc_min', 'wbc_max', 'wbc_mean']\n",
    "\n",
    "model=lgb.Booster(model_file=f'{root_location}/projects/Mortality_model/models/lgbm_model_20240429-083130.txt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### basic metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "      <th>SiteName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.956161</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.445563</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.934741</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ROC AUC</td>\n",
       "      <td>0.950812</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brier Score Loss</td>\n",
       "      <td>0.032848</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Metric     Value SiteName\n",
       "0          Accuracy  0.956161     RUSH\n",
       "1            Recall  0.445563     RUSH\n",
       "2         Precision  0.934741     RUSH\n",
       "3           ROC AUC  0.950812     RUSH\n",
       "4  Brier Score Loss  0.032848     RUSH"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test=icu_data[model_col]\n",
    "y_test=icu_data['isdeathdispo']\n",
    "\n",
    "y_pred_proba = model.predict(X_test)\n",
    "icu_data['pred_proba'] = y_pred_proba\n",
    "# Calculate metrics at default threshold (0.5)\n",
    "\n",
    "accuracy = accuracy_score(y_test, (y_pred_proba >= 0.5).astype(int))\n",
    "recall = recall_score(y_test, (y_pred_proba >= 0.5).astype(int))\n",
    "precision = precision_score(y_test, (y_pred_proba >= 0.5).astype(int))\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "brier_score = brier_score_loss(y_test, y_pred_proba)\n",
    "\n",
    "\n",
    "results_Metric = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Recall', 'Precision', 'ROC AUC', 'Brier Score Loss'],\n",
    "    'Value': [accuracy, recall, precision, roc_auc, brier_score],\n",
    "    'SiteName': [f'{site_name}'] * 5\n",
    "})\n",
    "\n",
    "results_Metric.to_csv(f'{output_directory}/result_metrics_{site_name}.csv',index=False)\n",
    "results_Metric"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### probablity table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_label</th>\n",
       "      <th>site_proba</th>\n",
       "      <th>Site_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.026871</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.012222</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.036550</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.031219</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   site_label   site_proba Site_name\n",
       "0            0    0.026871      RUSH\n",
       "1            0    0.012222      RUSH\n",
       "2            0    0.036550      RUSH\n",
       "3            0    0.003238      RUSH\n",
       "4            0    0.031219      RUSH"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_df_lgbm = pd.DataFrame({'site_label ':y_test, 'site_proba': y_pred_proba,'Site_name':f\"{site_name}\" })\n",
    "prob_df_lgbm.to_csv(f'{output_directory}/Model_probabilities_{site_name}.csv',index=False)\n",
    "prob_df_lgbm.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model fairness test accross 'race', 'ethnicity', 'sex'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(data, true_col, pred_prob_col, subgroup_cols):\n",
    "    results = []\n",
    "    total_count = len(data)\n",
    "\n",
    "    for subgroup_col in subgroup_cols:\n",
    "       \n",
    "        filtered_data = data.dropna(subset=[subgroup_col])\n",
    "        \n",
    "        for group in filtered_data[subgroup_col].unique():\n",
    "            subgroup_data = filtered_data[filtered_data[subgroup_col] == group]\n",
    "            group_count = len(subgroup_data)\n",
    "            proportion = group_count / total_count\n",
    "\n",
    "            if np.unique(subgroup_data[true_col]).size > 1:  # Check if both classes are present\n",
    "                auc = roc_auc_score(subgroup_data[true_col], subgroup_data[pred_prob_col])\n",
    "                tn, fp, fn, tp = confusion_matrix(subgroup_data[true_col], (subgroup_data[pred_prob_col] > 0.5).astype(int)).ravel()\n",
    "                ppv = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "                result = {'Subgroup': subgroup_col, 'Group': group, 'AUC': auc, 'PPV': ppv, 'Group Count': group_count, 'Total Count': total_count, 'Proportion': proportion, 'site_name':f'{site_name}'}\n",
    "            else:\n",
    "                result = {'Subgroup': subgroup_col, 'Group': group, 'AUC': 'Not defined', 'PPV': 'Not applicable', 'Group Count': group_count, 'Total Count': total_count, 'Proportion': proportion, 'site_name':f'{site_name}'}\n",
    "            \n",
    "            results.append(result)\n",
    "    \n",
    "   \n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "result_df = calculate_metrics(icu_data, 'isdeathdispo', 'pred_proba', ['race', 'ethnicity', 'sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subgroup</th>\n",
       "      <th>Group</th>\n",
       "      <th>AUC</th>\n",
       "      <th>PPV</th>\n",
       "      <th>Group Count</th>\n",
       "      <th>Total Count</th>\n",
       "      <th>Proportion</th>\n",
       "      <th>site_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>race</td>\n",
       "      <td>White</td>\n",
       "      <td>0.951074</td>\n",
       "      <td>0.951351</td>\n",
       "      <td>5621</td>\n",
       "      <td>14599</td>\n",
       "      <td>0.385026</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>race</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.952640</td>\n",
       "      <td>0.900826</td>\n",
       "      <td>2731</td>\n",
       "      <td>14599</td>\n",
       "      <td>0.187068</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>race</td>\n",
       "      <td>Black</td>\n",
       "      <td>0.947569</td>\n",
       "      <td>0.942105</td>\n",
       "      <td>5762</td>\n",
       "      <td>14599</td>\n",
       "      <td>0.394685</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>race</td>\n",
       "      <td>Asian</td>\n",
       "      <td>0.960787</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>485</td>\n",
       "      <td>14599</td>\n",
       "      <td>0.033221</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ethnicity</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>0.952598</td>\n",
       "      <td>0.941476</td>\n",
       "      <td>11621</td>\n",
       "      <td>14599</td>\n",
       "      <td>0.796013</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ethnicity</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>0.945992</td>\n",
       "      <td>0.906780</td>\n",
       "      <td>2874</td>\n",
       "      <td>14599</td>\n",
       "      <td>0.196863</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ethnicity</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.903353</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>104</td>\n",
       "      <td>14599</td>\n",
       "      <td>0.007124</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sex</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.950428</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>7779</td>\n",
       "      <td>14599</td>\n",
       "      <td>0.532845</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sex</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.951067</td>\n",
       "      <td>0.944223</td>\n",
       "      <td>6820</td>\n",
       "      <td>14599</td>\n",
       "      <td>0.467155</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Subgroup                   Group       AUC       PPV  Group Count  \\\n",
       "0       race                   White  0.951074  0.951351         5621   \n",
       "1       race                  Others  0.952640  0.900826         2731   \n",
       "2       race                   Black  0.947569  0.942105         5762   \n",
       "3       race                   Asian  0.960787  0.920000          485   \n",
       "4  ethnicity  Not Hispanic or Latino  0.952598  0.941476        11621   \n",
       "5  ethnicity      Hispanic or Latino  0.945992  0.906780         2874   \n",
       "6  ethnicity                  Others  0.903353  1.000000          104   \n",
       "7        sex                    Male  0.950428  0.925926         7779   \n",
       "8        sex                  Female  0.951067  0.944223         6820   \n",
       "\n",
       "   Total Count  Proportion site_name  \n",
       "0        14599    0.385026      RUSH  \n",
       "1        14599    0.187068      RUSH  \n",
       "2        14599    0.394685      RUSH  \n",
       "3        14599    0.033221      RUSH  \n",
       "4        14599    0.796013      RUSH  \n",
       "5        14599    0.196863      RUSH  \n",
       "6        14599    0.007124      RUSH  \n",
       "7        14599    0.532845      RUSH  \n",
       "8        14599    0.467155      RUSH  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.to_csv(f'{output_directory}/fairness_test_{site_name}.csv',index=False)\n",
    "result_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### thrshold check at site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_percentile(target_var, pred_proba):\n",
    "    #thr_list = [0.99,0.97, 0.95,0.90,0.80,0.70,0.60,0.50,0.40,0.30,0.20,0.10]\n",
    "    thr_list = np.arange(1, 0, -0.01)\n",
    "    col = ['N Percentile', 'Thr Value','TN','FP','FN','TP','Sensitivity','Specificity','PPV', 'NPV' ,'Recall','Accuracy','site_name']\n",
    "    result = pd.DataFrame(columns = col)\n",
    "    i = 0\n",
    "    \n",
    "    for thr in thr_list: \n",
    "        prob = pd.DataFrame()\n",
    "        prob['target_var'] = target_var\n",
    "        prob['pred_proba'] = pred_proba\n",
    "\n",
    "        thr_value = prob['pred_proba'].quantile(thr)\n",
    "        prob['pred_proba_bin'] = np.where(prob['pred_proba'] >= thr_value, 1, 0)\n",
    "        tn,fp,fn,tp = confusion_matrix(prob['target_var'], prob['pred_proba_bin']).ravel()\n",
    "\n",
    "        sensitivity = tp/(tp+fn)\n",
    "        specificity = tn/(tn+fp)\n",
    "        ppv = tp/(tp+fp)\n",
    "        npv = tn/(tn+fn)\n",
    "        recall = tp/(tp+fn)\n",
    "        acc = (tp+tn)/(tp+fn+tn+fp)\n",
    "        n_prec = 'Top '+ str(np.round((1 - thr) * 100,0))+ \"%\"\n",
    "        result.loc[i] = [n_prec,thr_value,tn,fp,fn,tp,sensitivity,specificity ,ppv,npv, recall, acc,f'{site_name}']\n",
    "        i+=1\n",
    "    return result\n",
    "topn=top_n_percentile(y_test,y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N Percentile</th>\n",
       "      <th>Thr Value</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TP</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>PPV</th>\n",
       "      <th>NPV</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>site_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top 0.0%</td>\n",
       "      <td>0.992234</td>\n",
       "      <td>13506</td>\n",
       "      <td>0</td>\n",
       "      <td>1092</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.925195</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>0.925200</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Top 1.0%</td>\n",
       "      <td>0.792637</td>\n",
       "      <td>13502</td>\n",
       "      <td>4</td>\n",
       "      <td>951</td>\n",
       "      <td>142</td>\n",
       "      <td>0.129918</td>\n",
       "      <td>0.999704</td>\n",
       "      <td>0.972603</td>\n",
       "      <td>0.934201</td>\n",
       "      <td>0.129918</td>\n",
       "      <td>0.934585</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Top 2.0%</td>\n",
       "      <td>0.647142</td>\n",
       "      <td>13494</td>\n",
       "      <td>12</td>\n",
       "      <td>813</td>\n",
       "      <td>280</td>\n",
       "      <td>0.256176</td>\n",
       "      <td>0.999112</td>\n",
       "      <td>0.958904</td>\n",
       "      <td>0.943175</td>\n",
       "      <td>0.256176</td>\n",
       "      <td>0.943489</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Top 3.0%</td>\n",
       "      <td>0.552321</td>\n",
       "      <td>13482</td>\n",
       "      <td>24</td>\n",
       "      <td>679</td>\n",
       "      <td>414</td>\n",
       "      <td>0.378774</td>\n",
       "      <td>0.998223</td>\n",
       "      <td>0.945205</td>\n",
       "      <td>0.952051</td>\n",
       "      <td>0.378774</td>\n",
       "      <td>0.951846</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Top 4.0%</td>\n",
       "      <td>0.465256</td>\n",
       "      <td>13462</td>\n",
       "      <td>44</td>\n",
       "      <td>553</td>\n",
       "      <td>540</td>\n",
       "      <td>0.494053</td>\n",
       "      <td>0.996742</td>\n",
       "      <td>0.924658</td>\n",
       "      <td>0.960542</td>\n",
       "      <td>0.494053</td>\n",
       "      <td>0.959107</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Top 5.0%</td>\n",
       "      <td>0.383166</td>\n",
       "      <td>13431</td>\n",
       "      <td>75</td>\n",
       "      <td>438</td>\n",
       "      <td>655</td>\n",
       "      <td>0.599268</td>\n",
       "      <td>0.994447</td>\n",
       "      <td>0.897260</td>\n",
       "      <td>0.968419</td>\n",
       "      <td>0.599268</td>\n",
       "      <td>0.964861</td>\n",
       "      <td>RUSH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  N Percentile  Thr Value     TN  FP    FN   TP  Sensitivity  Specificity  \\\n",
       "0     Top 0.0%   0.992234  13506   0  1092    1     0.000915     1.000000   \n",
       "1     Top 1.0%   0.792637  13502   4   951  142     0.129918     0.999704   \n",
       "2     Top 2.0%   0.647142  13494  12   813  280     0.256176     0.999112   \n",
       "3     Top 3.0%   0.552321  13482  24   679  414     0.378774     0.998223   \n",
       "4     Top 4.0%   0.465256  13462  44   553  540     0.494053     0.996742   \n",
       "5     Top 5.0%   0.383166  13431  75   438  655     0.599268     0.994447   \n",
       "\n",
       "        PPV       NPV    Recall  Accuracy site_name  \n",
       "0  1.000000  0.925195  0.000915  0.925200      RUSH  \n",
       "1  0.972603  0.934201  0.129918  0.934585      RUSH  \n",
       "2  0.958904  0.943175  0.256176  0.943489      RUSH  \n",
       "3  0.945205  0.952051  0.378774  0.951846      RUSH  \n",
       "4  0.924658  0.960542  0.494053  0.959107      RUSH  \n",
       "5  0.897260  0.968419  0.599268  0.964861      RUSH  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topn.to_csv(f'{output_directory}/Top_N_percentile_PPV_{site_name}.csv',index=False)\n",
    "topn.head(6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FineTune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 555, number of negative: 6744\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002521 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12985\n",
      "[LightGBM] [Info] Number of data points in the train set: 7299, number of used features: 74\n",
      "             Metric     Value SiteName FineTune\n",
      "0          Accuracy  0.956849     RUSH      Yes\n",
      "1            Recall  0.455390     RUSH      Yes\n",
      "2         Precision  0.917603     RUSH      Yes\n",
      "3           ROC AUC  0.953190     RUSH      Yes\n",
      "4  Brier Score Loss  0.032189     RUSH      Yes\n"
     ]
    }
   ],
   "source": [
    "if finetune:\n",
    "    train_data, test_data = train_test_split(icu_data, test_size=0.5, random_state=42)\n",
    "    X_train=train_data[model_col]\n",
    "    y_train=train_data['isdeathdispo']\n",
    "\n",
    "    #test\n",
    "    X_test=test_data[model_col]\n",
    "    y_test=test_data['isdeathdispo']\n",
    "\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "\n",
    "    params = {\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"binary_logloss\",\n",
    "        \"num_leaves\": 31,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"max_depth\":-1}\n",
    "    gbm = lgb.train(params, lgb_train, num_boost_round=10, init_model=f'{root_location}/projects/Mortality_model/models/lgbm_model_20240429-083130.txt')\n",
    "\n",
    "    y_pred_proba_ft = gbm.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, (y_pred_proba_ft >= 0.5).astype(int))\n",
    "    recall = recall_score(y_test, (y_pred_proba_ft >= 0.5).astype(int))\n",
    "    precision = precision_score(y_test, (y_pred_proba_ft >= 0.5).astype(int))\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba_ft)\n",
    "    brier_score = brier_score_loss(y_test, y_pred_proba_ft)\n",
    "\n",
    "\n",
    "    results_Metric = pd.DataFrame({\n",
    "        'Metric': ['Accuracy', 'Recall', 'Precision', 'ROC AUC', 'Brier Score Loss'],\n",
    "        'Value': [accuracy, recall, precision, roc_auc, brier_score],\n",
    "        'SiteName': [f'{site_name}'] * 5,\n",
    "        'FineTune': ['Yes'] * 5,\n",
    "    })\n",
    "    results_Metric.to_csv(f'{output_directory}/result_metrics_{site_name}_ft.csv',index=False)\n",
    "\n",
    "\n",
    "    model_filename = f\"{output_directory}/lgbm_model_{site_name}_ft.txt\"\n",
    "\n",
    "    # Save the model using LightGBM's built-in function\n",
    "    model.save_model(model_filename)\n",
    "\n",
    "    print(results_Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mortality_model)",
   "language": "python",
   "name": ".mortality_model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
